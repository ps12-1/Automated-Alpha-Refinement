# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Bvv4Gr9rsk1o_XKenswm9cxRBN60L0n

## ****
## **BRAIN API**

## **TABLE OF CONTENTS**

1.   **Preliminaries**
2.   **Custom Helper Functions**
3.   **Robustness Check**
4.   **Parameter Search Algorithms**  
    4.1.    **Grid Search**  
    4.2.    **Random Search**  
    4.3.    **Random Search with Pruning**  

5.   **GPT Based Alpha Research**
6.   **Evolutionary Algorithm Based Alpha Research**
7.   **Hiearchical Evolutionary Algorithm Based Alpha Research**
8.   **References**

## **1. PRELIMINARIES**

This section contains all the helper functions as provided by WorldQuant to faciliate easy use of the Brain API.
"""

#@title Helper Functions
import pandas as pd
import os
import time
import json
from typing import Optional
import requests
from urllib.parse import urljoin
import time
import json
import os
import getpass
from pathlib import Path
import pandas as pd
from multiprocessing.pool import ThreadPool
from functools import partial
import tqdm
import math
import random
import string
import os
! pip install --upgrade openai
from openai import OpenAI


DEFAULT_CONFIG = {
    "get_pnl": False,
    "get_stats": False,
    "save_pnl_file": False,
    "save_stats_file": False,
    "save_result_file": False,
    "check_submission": False,
    "check_self_corr": False,
    "check_prod_corr": False,
}


def make_clickable_alpha_id(alpha_id):
    """
    Make alpha_id clickable in dataframes
    So you can go to the platform to analyze simulation result
    """

    url = "https://platform.worldquantbrain.com/alpha/"
    return f'<a href="{url}{alpha_id}">{alpha_id}</a>'


def prettify_result(
    result, detailed_tests_view=False, clickable_alpha_id: bool = False
):
    """
    Combine needed results in one dataframe to analyze your alphas
    Sort by fitness absolute value
    """
    list_of_is_stats = [
        result[x]["is_stats"]
        for x in range(len(result))
        if result[x]["is_stats"] is not None
    ]
    is_stats_df = pd.concat(list_of_is_stats).reset_index(drop=True)
    is_stats_df = is_stats_df.sort_values("fitness", ascending=False)

    expressions = {
        result[x]["alpha_id"]: result[x]["simulate_data"]["regular"]
        for x in range(len(result))
        if result[x]["is_stats"] is not None
    }
    expression_df = pd.DataFrame(
        list(expressions.items()), columns=["alpha_id", "expression"]
    )

    list_of_is_tests = [
        result[x]["is_tests"]
        for x in range(len(result))
        if result[x]["is_tests"] is not None
    ]
    is_tests_df = pd.concat(list_of_is_tests).reset_index(drop=True)
    if detailed_tests_view:
        cols = ["limit", "result", "value"]
        is_tests_df["details"] = is_tests_df[cols].to_dict(orient="records")
        is_tests_df = is_tests_df.pivot(
            index="alpha_id", columns="name", values="details"
        ).reset_index()
    else:
        is_tests_df = is_tests_df.pivot(
            index="alpha_id", columns="name", values="result"
        ).reset_index()

    alpha_stats = pd.merge(is_stats_df, expression_df, on="alpha_id")
    alpha_stats = pd.merge(alpha_stats, is_tests_df, on="alpha_id")
    alpha_stats = alpha_stats.drop(
        columns=alpha_stats.columns[(alpha_stats == "PENDING").any()]
    )
    alpha_stats.columns = alpha_stats.columns.str.replace(
        "(?<=[a-z])(?=[A-Z])", "_", regex=True
    ).str.lower()
    if clickable_alpha_id:
        return alpha_stats.style.format({"alpha_id": make_clickable_alpha_id})
    return alpha_stats


def concat_pnl(result):
    """
    Combine needed results in one dataframe to analyze pnls of your alphas
    """
    list_of_pnls = [
        result[x]["pnl"]
        for x in range(len(result))
        if result[x]["pnl"] is not None
    ]
    pnls_df = pd.concat(list_of_pnls).reset_index()

    return pnls_df


def concat_is_tests(result):
    is_tests_list = [
        result[x]["is_tests"]
        for x in range(len(result))
        if result[x]["is_tests"] is not None
    ]
    is_tests_df = pd.concat(is_tests_list).reset_index(drop=True)
    return is_tests_df


def save_simulation_result(result):
    """
    Dump simulation result to folder simulation_results
    to json file
    """

    alpha_id = result["id"]
    region = result["settings"]["region"]
    folder_path = "simulation_results/"
    file_path = os.path.join(folder_path, f"{alpha_id}_{region}")

    os.makedirs(folder_path, exist_ok=True)

    with open(file_path, "w") as file:
        json.dump(result, file)

def set_alpha_properties(
    s,
    alpha_id,
    name: str = None,
    color: str = None,
    selection_desc: str = "None",
    combo_desc: str = "None",
    tags: str = ["iiittm_tag"],
):
    """
    Function changes alpha's description parameters
    """

    params = {
        "color": color,
        "name": name,
        "tags": tags,
        "category": None,
        "regular": {"description": None},
        "combo": {"description": combo_desc},
        "selection": {"description": selection_desc},
    }
    response = s.patch(
        "https://api.worldquantbrain.com/alphas/" + alpha_id, json=params
    )



def save_pnl(pnl_df, alpha_id, region):
    """
    Dump pnl to folder alphas_pnl
    to csv file
    """

    folder_path = "alphas_pnl/"
    file_path = os.path.join(folder_path, f"{alpha_id}_{region}")
    os.makedirs(folder_path, exist_ok=True)

    pnl_df.to_csv(file_path)


def save_yearly_stats(yearly_stats, alpha_id, region):
    """
    Dump yearly-stats to folder yearly_stats
    to csv file
    """

    folder_path = "yearly_stats/"
    file_path = os.path.join(folder_path, f"{alpha_id}_{region}")
    os.makedirs(folder_path, exist_ok=True)

    yearly_stats.to_csv(file_path, index=False)


def get_alpha_pnl(s, alpha_id):
    """
    Function gets alpha pnl of simulation
    """

    while True:
        result = s.get(
            "https://api.worldquantbrain.com/alphas/" + alpha_id + "/recordsets/pnl"
        )
        if "retry-after" in result.headers:
            time.sleep(float(result.headers["Retry-After"]))
        else:
            break
    pnl = result.json().get("records", 0)
    if pnl == 0:
        return pd.DataFrame()
    pnl_df = (
        pd.DataFrame(pnl, columns=["Date", "Pnl"])
        .assign(
            alpha_id=alpha_id, Date=lambda x: pd.to_datetime(x.Date, format="%Y-%m-%d")
        )
        .set_index("Date")
    )
    return pnl_df


def get_alpha_yearly_stats(s, alpha_id):
    """
    Function gets yearly-stats of simulation
    """

    while True:
        result = s.get(
            "https://api.worldquantbrain.com/alphas/"
            + alpha_id
            + "/recordsets/yearly-stats"
        )
        if "retry-after" in result.headers:
            time.sleep(float(result.headers["Retry-After"]))
        else:
            break
    stats = result.json()

    if stats.get("records", 0) == 0:
        return pd.DataFrame()
    columns = [dct["name"] for dct in stats["schema"]["properties"]]
    yearly_stats_df = pd.DataFrame(stats["records"], columns=columns).assign(alpha_id=alpha_id)
    return yearly_stats_df

def get_datasets(
    s,
    instrument_type: str = 'EQUITY',
    region: str = 'USA',
    delay: int = 1,
    universe: str = 'TOP3000'
):
    url = "https://api.worldquantbrain.com/data-sets?" +\
        f"instrumentType={instrument_type}&region={region}&delay={str(delay)}&universe={universe}"
    result = s.get(url)
    datasets_df = pd.DataFrame(result.json()['results'])
    return datasets_df


def get_datafields(
    s,
    instrument_type: str = 'EQUITY',
    region: str = 'USA',
    delay: int = 1,
    universe: str = 'TOP3000',
    dataset_id: str = '',
    search: str = ''
):
    if len(search) == 0:
        url_template = "https://api.worldquantbrain.com/data-fields?" +\
            f"&instrumentType={instrument_type}" +\
            f"&region={region}&delay={str(delay)}&universe={universe}&dataset.id={dataset_id}&limit=50" +\
            "&offset={x}"
        count = s.get(url_template.format(x=0)).json()['count']
    else:
        url_template = "https://api.worldquantbrain.com/data-fields?" +\
            f"&instrumentType={instrument_type}" +\
            f"&region={region}&delay={str(delay)}&universe={universe}&limit=50" +\
            f"&search={search}" +\
            "&offset={x}"
        count = 100

    datafields_list = []
    for x in range(0, count, 50):
        datafields = s.get(url_template.format(x=x))
        datafields_list.append(datafields.json()['results'])

    datafields_list_flat = [item for sublist in datafields_list for item in sublist]

    datafields_df = pd.DataFrame(datafields_list_flat)
    return datafields_df

#@title Start Session

def get_credentials():
    """
    Function gets json with platform credentials if exists,
    or asks to enter new ones
    """

    credential_email = os.environ.get('BRAIN_CREDENTIAL_EMAIL')
    credential_password = os.environ.get('BRAIN_CREDENTIAL_PASSWORD')

    credentials_folder_path = os.path.join(os.path.expanduser("~"), "secrets")
    credentials_file_path = os.path.join(credentials_folder_path, "platform-brain.json")

    if (
        Path(credentials_file_path).exists()
        and os.path.getsize(credentials_file_path) > 2
    ):
        with open(credentials_file_path) as file:
            data = json.loads(file.read())
    else:
        os.makedirs(credentials_folder_path, exist_ok=True)
        if credential_email and credential_password:
            email = credential_email
            password = credential_password
        else:
            email = input("Email:\n")
            password = getpass.getpass(prompt="Password:")
        data = {"email": email, "password": password}
        with open(credentials_file_path, "w") as file:
            json.dump(data, file)
    return (data["email"], data["password"])


def start_session():

    """
    Function sign in to platform
    and checks credentials
    and returns session object
    """

    s = requests.Session()
    s.auth = get_credentials()
    r = s.post("https://api.worldquantbrain.com/authentication")

    if r.status_code == requests.status_codes.codes.unauthorized:
        if r.headers["WWW-Authenticate"] == "persona":
            print(
                "Complete biometrics authentication and press any key to continue: \n"
                + urljoin(r.url, r.headers["Location"]) + "\n"
            )
            input()
            s.post(urljoin(r.url, r.headers["Location"]))

            while True:
                if s.post(urljoin(r.url, r.headers["Location"])).status_code != 201:
                    input("Biometrics authentication is not complete. Please try again and press any key when completed \n")
                else:
                    break
        else:
            print("\nIncorrect email or password\n")
            with open(
                os.path.join(os.path.expanduser("~"), "secrets/platform-brain.json"),
                "w",
            ) as file:
                json.dump({}, file)
            return start_session()
    return s

s = start_session()

#@title Predefined Functions

def check_session_timeout(s):
    """
    Function checks session time out
    """

    authentication_url = "https://api.worldquantbrain.com/authentication"
    try:
        result = s.get(authentication_url).json()["token"]["expiry"]
        return result
    except:
        return 0


def generate_alpha(
    regular: str,
    region: str = "USA",
    universe: str = "TOP3000",
    neutralization: str = "INDUSTRY",
    delay: int = 1,
    decay: int = 0,
    truncation: float = 0.08,
    nan_handling: str = "OFF",
    unit_handling: str = "VERIFY",
    pasteurization: str = "ON",
    visualization: bool = False,
):
    """
    Function generates data to use in simulation
    has default parameters
    """

    simulation_data = {
        "type": "REGULAR",
        "settings": {
            "nanHandling": nan_handling,
            "instrumentType": "EQUITY",
            "delay": delay,
            "universe": universe,
            "truncation": truncation,
            "unitHandling": unit_handling,
            "pasteurization": pasteurization,
            "region": region,
            "language": "FASTEXPR",
            "decay": decay,
            "neutralization": neutralization,
            "visualization": visualization,
        },
        "regular": regular,
    }
    return simulation_data


def start_simulation(
    s, simulate_data
):
    simulate_response = s.post(
        "https://api.worldquantbrain.com/simulations", json=simulate_data
    )
    return simulate_response


def simulation_progress(s,
    simulate_response,
):

    if simulate_response.status_code // 100 != 2:
        print(simulate_response.text)
        return {"completed": False, "result": {}}

    simulation_progress_url = simulate_response.headers["Location"]
    error_flag = False
    while True:
        simulation_progress = s.get(simulation_progress_url)
        if simulation_progress.headers.get("Retry-After", 0) == 0:
            if simulation_progress.json().get("status", "ERROR") == "ERROR":
                error_flag = True
            break

        time.sleep(float(simulation_progress.headers["Retry-After"]))

    if error_flag:

        print("An error occurred")
        if "message" in simulation_progress.json():
            print(simulation_progress.json()["message"])
        return {"completed": False, "result": {}}

    alpha = simulation_progress.json().get("alpha", 0)
    if alpha == 0:
        return {"completed": False, "result": {}}
    simulation_result = get_simulation_result_json(s, alpha)
    return {"completed": True, "result": simulation_result}



def multisimulation_progress(s,
    simulate_response,
):

    if simulate_response.status_code // 100 != 2:
        print(simulate_response.text)
        return {"completed": False, "result": {}}

    simulation_progress_url = simulate_response.headers["Location"]
    error_flag = False
    while True:
        simulation_progress = s.get(simulation_progress_url)
        if simulation_progress.headers.get("Retry-After", 0) == 0:
            if simulation_progress.json().get("status", "ERROR") == "ERROR":
                error_flag = True
            break

        time.sleep(float(simulation_progress.headers["Retry-After"]))

    if error_flag:
        print("An error occurred")
        if "message" in simulation_progress.json():
            print(simulation_progress.json()["message"])
        return {"completed": False, "result": {}}


    children = simulation_progress.json().get("children", 0)
    if len(children) == 0:
        return {"completed": False, "result": {}}
    children_list = []
    for child in children:
        child_progress = s.get("https://api.worldquantbrain.com/simulations/" + child)
        alpha = child_progress.json()["alpha"]
        child_result = get_simulation_result_json(s, alpha)
        children_list.append(child_result)
    return {"completed": True, "result": children_list}


def get_prod_corr(s, alpha_id):
    """
    Function gets alpha's prod correlation
    and save result to dataframe
    """

    while True:
        result = s.get(
            "https://api.worldquantbrain.com/alphas/" + alpha_id + "/correlations/prod"
        )
        if "retry-after" in result.headers:
            time.sleep(float(result.headers["Retry-After"]))
        else:
            break
    if result.json().get("records", 0) == 0:
        return pd.DataFrame()
    columns = [dct["name"] for dct in result.json()["schema"]["properties"]]
    prod_corr_df = pd.DataFrame(result.json()["records"], columns=columns).assign(alpha_id=alpha_id)

    return prod_corr_df


def check_prod_corr_test(s, alpha_id, threshold: float = 0.7):
    """
    Function checks if alpha's prod_corr test passed
    Saves result to dataframe
    """

    prod_corr_df = get_prod_corr(s, alpha_id)
    value = prod_corr_df[prod_corr_df.alphas > 0]["max"].max()
    # result = [
    #     {"test": "PROD_CORRELATION", "result": "PASS" if value <= threshold else "FAIL", "limit": threshold, "value": value, "alpha_id": alpha_id}
    # ]
    result = []
    if value<=threshold :
        result = [
            {"test": "PROD_CORRELATION", "result": "PASS" , "limit": threshold, "value": value, "alpha_id": alpha_id}
        ]
    # return pd.DataFrame(result)
    return result


def get_self_corr(s, alpha_id):
    """
    Function gets alpha's self correlation
    and save result to dataframe
    """

    while True:

        result = s.get(
            "https://api.worldquantbrain.com/alphas/" + alpha_id + "/correlations/self"
        )
        if "retry-after" in result.headers:
            time.sleep(float(result.headers["Retry-After"]))
        else:
            break
    if result.json().get("records", 0) == 0:
        return pd.DataFrame()

    records_len = len(result.json()["records"])
    if records_len == 0:
        return pd.DataFrame()

    columns = [dct["name"] for dct in result.json()["schema"]["properties"]]
    self_corr_df = pd.DataFrame(result.json()["records"], columns=columns).assign(alpha_id=alpha_id)

    return self_corr_df


def check_self_corr_test(s, alpha_id, threshold: float = 0.7):
    """
    Function checks if alpha's self_corr test passed
    Saves result to dataframe
    """

    self_corr_df = get_self_corr(s, alpha_id)
    if self_corr_df.empty:
        result = [{"test": "SELF_CORRELATION", "result": "PASS", "limit": threshold, "value": 0, "alpha_id": alpha_id}]
    else:
        value = self_corr_df["correlation"].max()
        result = [
            {
                "test": "SELF_CORRELATION",
                "result": "PASS" if value < threshold else "FAIL",
                "limit": threshold,
                "value": value,
                "alpha_id": alpha_id
            }
        ]
    return pd.DataFrame(result)



def get_check_submission(s, alpha_id):
    """
    Function gets alpha's check submission checks
    and returns result in dataframe
    """

    while True:
        result = s.get("https://api.worldquantbrain.com/alphas/" + alpha_id + "/check")
        if "retry-after" in result.headers:
            time.sleep(float(result.headers["Retry-After"]))
        else:
            break
    if result.json().get("is", 0) == 0:
        return pd.DataFrame()

    checks_df = pd.DataFrame(
            result.json()["is"]["checks"]
    ).assign(alpha_id=alpha_id)

    if 'year' in checks_df:
        ladder_dict = [checks_df.loc[checks_df.index[checks_df['name']=='IS_LADDER_SHARPE']][['value', 'year']].iloc[0].to_dict()]
        checks_df.at[checks_df.index[checks_df['name']=='IS_LADDER_SHARPE'], 'value'] = ladder_dict
        checks_df.drop(['endDate', 'startDate', 'year'], axis=1, inplace=True)

    return checks_df


def performance_comparison(s, alpha_id, team_id:Optional[str] = None, competition:Optional[str] = 'IIITTM12.0'):
    """
    Returns performance comparison for merged performance check
    """
    if competition is not None:
        part_url = f'competitions/{competition}'
    elif team_id is not None:
        part_url = f'teams/{team_id}'
    else:
        part_url = 'users/self'
    while True:
        result = s.get(
            f"https://api.worldquantbrain.com/{part_url}/alphas/" + alpha_id + "/before-and-after-performance"
        )
        if "retry-after" in result.headers:
            time.sleep(float(result.headers["Retry-After"]))
        else:
            break
    if result.json().get("stats", 0) == 0:
        return {}
    if result.status_code != 200:
        return {}

    return result.json()


def submit_alpha(s, alpha_id):
    """
    Function submits an alpha
    This function is not used anywhere
    """
    result = s.post("https://api.worldquantbrain.com/alphas/" + alpha_id + "/submit")
    while True:
        if "retry-after" in result.headers:
            time.sleep(float(result.headers["Retry-After"]))
            result = s.get("https://api.worldquantbrain.com/alphas/" + alpha_id + "/submit")
        else:
            break
    return result.status_code == 200


def get_simulation_result_json(s, alpha_id):
    return s.get("https://api.worldquantbrain.com/alphas/" + alpha_id).json()


def simulate_single_alpha(
    s,
    simulate_data,
):
    """
    To simulate single alpha
    """

    # if check_session_timeout(s) < 1000:
    #     s = start_session()

    simulate_response = start_simulation(s, simulate_data)
    simulation_result = simulation_progress(s, simulate_response)

    if not simulation_result["completed"]:
        return {'alpha_id': None, 'simulate_data': simulate_data}
    set_alpha_properties(s, simulation_result["result"]["id"])
    return {'alpha_id': simulation_result["result"]["id"], 'simulate_data': simulate_data}


def simulate_multi_alpha(
    s,
    simulate_data_list,
):
    """
    To simulate single alpha
    """

    # if check_session_timeout(s) < 1000:
    #     s = start_session()
    if len(simulate_data_list) == 1:
        return [simulate_single_alpha(s, simulate_data_list[0])]
    simulate_response = start_simulation(s, simulate_data_list)
    simulation_result = multisimulation_progress(s, simulate_response)

    if not simulation_result["completed"]:
        return [{'alpha_id': None, 'simulate_data': x} for x in simulate_data_list]
    result = [{"alpha_id": x["id"], "simulate_data": {"type": x["type"], "settings": x["settings"], "regular": x["regular"]["code"]}} for x in simulation_result["result"]]
    _ = [set_alpha_properties(s, x["id"]) for x in simulation_result["result"]]
    return result


def get_specified_alpha_stats(
    s,
    alpha_id,
    simulate_data,
    get_pnl: bool = False,
    get_stats: bool = False,
    save_pnl_file: bool = False,
    save_stats_file: bool = False,
    save_result_file: bool = False,
    check_submission: bool = False,
    check_self_corr: bool = False,
    check_prod_corr: bool = False,
):
    """
    Master-Function to get specified in config statistics

    """
    pnl = None
    stats = None

    if alpha_id is None:
        return {'alpha_id': None, 'simulate_data': simulate_data, 'is_stats': None, 'pnl': pnl, 'stats': stats, 'is_tests': None}

    result = get_simulation_result_json(s, alpha_id)
    region = result["settings"]["region"]
    is_stats = pd.DataFrame([{key: value for key, value in result['is'].items() if key!='checks'}]).assign(alpha_id=alpha_id)

    if get_pnl:
        pnl = get_alpha_pnl(s, alpha_id)
    if get_stats:
        stats = get_alpha_yearly_stats(s, alpha_id)

    if save_result_file:
        save_simulation_result(result)
    if save_pnl_file and get_pnl:
        save_pnl(pnl, alpha_id, region)
    if save_stats_file and get_stats:
        save_yearly_stats(stats, alpha_id, region)

    is_tests = pd.DataFrame(
        result["is"]["checks"]
    ).assign(alpha_id=alpha_id)

    if check_submission:
        is_tests = get_check_submission(s, alpha_id)

        return {'alpha_id': alpha_id, 'simulate_data': simulate_data, 'is_stats': is_stats, 'pnl': pnl, 'stats': stats, 'is_tests': is_tests}

    if check_self_corr and not check_submission:
        self_corr_test = check_self_corr_test(s, alpha_id)
        is_tests = (
            is_tests.append(self_corr_test, ignore_index=True, sort=False)
            .drop_duplicates(subset=["test"], keep="last")
            .reset_index(drop=True)
        )
    if check_prod_corr and not check_submission:
        prod_corr_test = check_prod_corr_test(s, alpha_id)
        is_tests = (
            is_tests.append(prod_corr_test, ignore_index=True, sort=False)
            .drop_duplicates(subset=["test"], keep="last")
            .reset_index(drop=True)
        )

    return {'alpha_id': alpha_id, 'simulate_data': simulate_data, 'is_stats': is_stats, 'pnl': pnl, 'stats': stats, 'is_tests': is_tests}


def simulate_alpha_list(
    s,
    alpha_list,
    limit_of_concurrent_simulations=3,
    simulation_config=DEFAULT_CONFIG,
):
    result_list = []

    with ThreadPool(limit_of_concurrent_simulations) as pool:

        with tqdm.tqdm(total=len(alpha_list)) as pbar:

            for result in pool.imap_unordered(
                partial(simulate_single_alpha, s), alpha_list
            ):
                result_list.append(result)
                pbar.update()

    stats_list_result = []
    func = lambda x: get_specified_alpha_stats(s, x['alpha_id'], x['simulate_data'], **simulation_config)
    with ThreadPool(3) as pool:
        for result in pool.map(
            func, result_list
        ):
            stats_list_result.append(result)

    return stats_list_result

def simulate_alpha_list_multi(s,
    alpha_list,
    limit_of_concurrent_simulations=3,
    limit_of_multi_simulations=3,
    simulation_config=DEFAULT_CONFIG,
):
    if (limit_of_multi_simulations<2) or (limit_of_multi_simulations>10):
        print('Warning, limit of multi-simulation should be 2..10')
        limit_of_multi_simulations = 3
    if len(alpha_list)<10:
        print('Warning, list of alphas too short, single concurrent simulations will be used instead of multisimulations')
        return simulate_alpha_list(s, alpha_list, simulation_config=simulation_config)

    tasks = [alpha_list[i:i + limit_of_multi_simulations] for i in range(0, len(alpha_list), limit_of_multi_simulations)]
    result_list = []

    with ThreadPool(limit_of_concurrent_simulations) as pool:

        with tqdm.tqdm(total=len(tasks)) as pbar:
            for result in pool.imap_unordered(
                partial(simulate_multi_alpha, s), tasks
            ):
                result_list.append(result)
                pbar.update()
    result_list_flat = [item for sublist in result_list for item in sublist]

    stats_list_result = []
    func = lambda x: get_specified_alpha_stats(s, x['alpha_id'], x['simulate_data'], **simulation_config)
    with ThreadPool(3) as pool:
        for result in pool.map(
            func, result_list_flat
        ):
            stats_list_result.append(result)
    return stats_list_result

"""## **2. CUSTOM HELPER FUNCTIONS** ##

This section contains modifications of the provided helper functions along with new functions made by our team for use throughout the notebook.

**Prettify Result Mod**

The below function is a slight modification of the original "prettify result" function, which falicitates returning a dataframe of only those alphas which has utmost 2 Fail Cases out of 9 Test Cases.
"""

#@title Prettify Result Mod
def prettify_result_mod(
    result, detailed_tests_view=False, clickable_alpha_id: bool = False
):
    list_of_is_stats = [
        result[x]["is_stats"]
        for x in range(len(result))
        if result[x]["is_stats"] is not None
    ]

    if(len(list_of_is_stats)==0):
        print("No stats. Some error occured in the operators")
        return pd.DataFrame()

    is_stats_df = pd.concat(list_of_is_stats).reset_index(drop=True)
    is_stats_df = is_stats_df.sort_values("fitness", ascending=False)

    expressions = {
        result[x]["alpha_id"]: result[x]["simulate_data"]["regular"]
        for x in range(len(result))
        if result[x]["is_stats"] is not None
    }
    expression_df = pd.DataFrame(
        list(expressions.items()), columns=["alpha_id", "expression"]
    )

    list_of_is_tests = [
        result[x]["is_tests"]
        for x in range(len(result))
        if result[x]["is_tests"] is not None
    ]

    """
    USE THE BELOW CODE IF YOU WANT ALL CASES PASSED.
    The below code will check if there is any failed constraint in the given list of alphas.
    If there is any failed constraint, the output dataframe will not be containing the concerned alpha.
    """
    # final_list_of_is_tests = []
    # for i in list_of_is_tests:
    #   flag = True
    #   for s_test in i.loc[:,"result"]:
    #     if s_test == 'FAIL':
    #       flag = False
    #       break
    #   if flag:
    #     final_list_of_is_tests.append(i)

    """
    USE THE BELOW CODE IF YOU WANT ALL CASES PASSED.
    The below code will check if there are more than 2 failed constraints in the given list of alphas.
    The output dataframe will only contain the alphas which has atmost 2 failed constraints. (DEFAULT)
    """
    final_list_of_is_tests = []
    for i in list_of_is_tests:
      fail_count = 0
      for s_test in i.loc[:,"result"]:
        if s_test == 'FAIL':
          fail_count += 1
      if fail_count<=2:
        final_list_of_is_tests.append(i)

    list_of_is_tests = final_list_of_is_tests

    if len(list_of_is_tests)==0:
      print("No Variations passed all the tests")
      return pd.DataFrame()

    is_tests_df = pd.concat(list_of_is_tests).reset_index(drop=True)
    if detailed_tests_view:
        cols = ["limit", "result", "value"]
        is_tests_df["details"] = is_tests_df[cols].to_dict(orient="records")
        is_tests_df = is_tests_df.pivot(
            index="alpha_id", columns="name", values="details"
        ).reset_index()
    else:
        is_tests_df = is_tests_df.pivot(
            index="alpha_id", columns="name", values="result"
        ).reset_index()

    alpha_stats = pd.merge(is_stats_df, expression_df, on="alpha_id")
    alpha_stats = pd.merge(alpha_stats, is_tests_df, on="alpha_id")
    alpha_stats = alpha_stats.drop(
        columns=alpha_stats.columns[(alpha_stats == "PENDING").any()]
    )
    alpha_stats.columns = alpha_stats.columns.str.replace(
        "(?<=[a-z])(?=[A-Z])", "_", regex=True
    ).str.lower()
    if clickable_alpha_id:
        return alpha_stats.style.format({"alpha_id": make_clickable_alpha_id})
    return alpha_stats

"""**Get Datafields from a Dataset**

The below function takes name of a dataset and a type field as input. It fetches all the datafields of the input dataset, and filters out the datafields of the particular type field if specified in the function call.
"""

# @title Get Datafields from a Dataset
def get_datafields_from_dataset(dataset_name, type_field = None):
  datasets = get_datasets(s)
  dataset = datasets[datasets['name'] == dataset_name]
  dataset_id = dataset.iloc[0]['id']
  datafields = get_datafields(s, dataset_id = dataset_id)
  if type_field is not None:
    datafields = datafields[datafields['type'] == type_field]
  return datafields['id'].tolist()


n_list = ["SUBINDUSTRY","INDUSTRY","COUNTRY","MARKET","SECTOR"]
d_list = [0,4,8,12,16,20,40]
u_list = ["TOP3000","TOP1000","TOP500","ILLIQUID_MINVOL1M"]

"""**Compare Two Strings**

The below function takes two strings as input. It checks the equality of the raw text of the both the strings ignoring spaces at all. It also ignores the cases of the strings while checking.
"""

#@title Compare Two Strings
def compare(s1, s2):
    s3 = str(s1)
    s4 = str(s2)
    s3 = s3.replace(" ","")
    s4 = s4.replace(" ","")
    if s3.casefold()==s4.casefold():
        return True
    return False

"""**Get Fitness**
The below function takes sharpe, turnover, drawdown, margin, long count and short count as input. It computes the Fitness of the given input stats according to the formula

$$
Fitness = \frac{sharpe^2 \ . \ returns}{(turnover+1) \ . drawdown}
$$
"""

#@title Get Fitness
def get_fitness(sharpe, turnover, drawdown, returns, long_count, short_count):
  fitness = -1e9
  if (long_count+short_count)<500 or short_count==0:
    pass
  elif turnover==0 or drawdown==0 or (long_count/short_count)<0.2 or (long_count/short_count)>5:
    pass
  elif sharpe>0 and returns>0:
    fitness = ((sharpe**2)*returns)/(((turnover+1.0))*drawdown)
  else:
    fitness = -1 * ((sharpe**2)*returns)/(((turnover+1.0))*drawdown)
  return fitness

"""**Managing Turnover**

After making an alpha, if the turnover is high, there is a series of operators which can be applied on different settings on the given alpha to reduce the turnover. This function automates this process and applies all of these operators at all predefined settings, outputting any improvement if found.
"""

#@title Managing Turnover
def manage_turnover(alpha, settings):
    cur_d = d_list.index(settings["decay"])
    cur_n= n_list.index(settings["neutralization"])
    cur_u = u_list.index(settings["universe"])
    alpha_result = simulate_single_alpha(s,generate_alpha(alpha, universe=u_list[cur_u],decay=d_list[cur_d],neutralization=n_list[cur_n]))
    alpha_turnover = alpha_result["is_stats"]["turnover"]
    tradewhen_conditions = ["volume>ts_mean(volume,20)","rank(ts_std_dev(returns,22))>0.5"]
    hump_constants = []
    tsdecay_lookback = []
    tsdecay_factor = []
    alpha_list = []
    for i in tradewhen_conditions:
      alpha_list.append("trade_when ( " + i + " , " + alpha + " , " + " -1 )")
    for i in tsdecay_lookback:
      for j in tsdecay_factor:
        alpha_list.append("ts_decay_exp_window ( " + alpha + " , " + i + " , " + j + " )")
    for i in hump_constants:
      alpha_list.append("hump ( " + alpha + " , " + " hump = " + i)

    alpha_gen_list = [generate_alpha(x, universe = settings["universe"], decay = settings["decay"] , neutralization = settings["neutralization"]) for x in alpha_list]
    for i in alpha_list:
      result = simulate_alpha_list(s, [generate_alpha(i, universe = settings["universe"], decay = settings["decay"] , neutralization = settings["neutralization"])])
      df = prettify_result_mod(result, clickable_alpha_id = True) ## Make sure 'all cases pass' code is uncommented in the function
      try:
        df = df.data
      except:
        continue
      if df[0]["turnover"]<alpha_turnover :
        display(df)

"""## **3. ROBUSTNESS CHECK**

After making an alpha, we perform a series of tests on the alpha to make sure it is not sensitive to slight changes in settings or the alpha itself. These test ensures that the alpha is not overfitted and would perform well in the OS. The list of the all tests performed are as follows -

*   Subuniverse and superuniverse test
*   Different neutralization settings
*   Slightly changing decay values
*   Wrapping the alpha with the rank operator
*   Converting the alpha's output to binary (+1, -1)


"""

def stress_testing(alpha, settings):
    n_list = ["SUBINDUSTRY","INDUSTRY","COUNTRY","MARKET","SECTOR"]
    d_list = [0,4,8,10,16,20,40]
    u_list = ["TOP3000","TOP1000","TOP500","ILLIQUID_MINVOL1M"]
    cur_d = d_list.index(settings["decay"])
    cur_n= n_list.index(settings["neutralization"])
    cur_u = u_list.index(settings["universe"])
    alpha_result = simulate_alpha_list(s,[generate_alpha(alpha, universe=u_list[cur_u],decay=d_list[cur_d],neutralization=n_list[cur_n])])
    alpha_sharpe = alpha_result[0]["is_stats"]["sharpe"][0]
    print(alpha_sharpe)
    alpha_list=[]
    for i in range(-1,2,2):
      if cur_d+i>=0 and cur_d+i<len(d_list):
          alpha_list.append(generate_alpha(alpha,universe=u_list[cur_u],decay=d_list[cur_d+i],neutralization=n_list[cur_n]))

      if cur_u+i>=0 and cur_u+i<len(u_list):
          alpha_list.append(generate_alpha(alpha,universe=u_list[cur_u+i],decay=d_list[cur_d],neutralization=n_list[cur_n]))

      if cur_n+i>=0 and cur_n+i<len(n_list):
          alpha_list.append(generate_alpha(alpha,universe=u_list[cur_u],decay=d_list[cur_d],neutralization=n_list[cur_n+i]))

    alpha_list.append(generate_alpha(str("rank ( " + alpha + " )"),universe=u_list[cur_u],decay=d_list[cur_d],neutralization=n_list[cur_n]))
    alpha_list.append(generate_alpha(str("rank ( " + alpha + " ) > 0.5 - rank ( " + alpha + " ) < 0.5"),universe=u_list[cur_u],decay=d_list[cur_d],neutralization=n_list[cur_n]))
    stats_list_result = simulate_alpha_list_multi(s, alpha_list, limit_of_multi_simulations=5)
    for i in range(len(stats_list_result)):
      if stats_list_result[i]["is_stats"] is not None:
        cur_alpha_sharpe = stats_list_result[i]["is_stats"]["sharpe"][0]
        if cur_alpha_sharpe <= (0.6*alpha_sharpe):
          print("Case Failed")
          print(stats_list_result[i]['simulate_data']['regular'])

          print("Settings: ", stats_list_result[i]["simulate_data"]["settings"]["universe"], stats_list_result[i]["simulate_data"]["settings"]["decay"], stats_list_result[i]["simulate_data"]["settings"]["neutralization"])
          print(cur_alpha_sharpe)
          print()
          print()

        else:
          print("Case Passed")
          print(stats_list_result[i]['simulate_data']['regular'])
          print("Settings: ", stats_list_result[i]["simulate_data"]["settings"]["universe"], stats_list_result[i]["simulate_data"]["settings"]["decay"], stats_list_result[i]["simulate_data"]["settings"]["neutralization"])
          print()
          print()
      else:
        print("Stats is none")

#@title Example Run
alpha = "zscore((high+low)/2 - close)"
settings = {"universe":"TOP3000", "neutralization":"SUBINDUSTRY","decay":10}
stress_testing(alpha, settings)

"""## **4. PARAMETER SEARCH ALGORTIHMS**

After ideating an alpha expression, the settings along with operators, tresholds and datafields, need to be varied to arrive at the most optimal values. The challenge of this task is to perform this search in manageable amount of time while also making sure the alpha's do not get overfitted. This section contains multiple algorithms to perform this task, each an improvement over the previous.

**Settings Variations**

The below cells contains 3 lists - neutralisation list, decay list and universe list. This contains all the settings the users wants to variate on. If the user wants to variate on a subset of the settings, the user can edit in the below cell.
"""

#@title Settings Variations

"""
The below commented code contains the full list of possible settings.
The user can edit these lists according to his/her needs.
"""
# n_list = ["SUBINDUSTRY","INDUSTRY","COUNTRY","MARKET","SECTOR"]
# d_list = [0,4,8,12,16,20,40]
# u_list = ["TOP3000","TOP1000","TOP500","ILLIQUID_MINVOL1M"]

n_list = ["SUBINDUSTRY",]
d_list = [10,20]
u_list = ["TOP3000",]

"""**Operators Variations**

The below cells contains lists of all the different type of operators that the user can variate on, in the search space. If the user does not define a custom list of variations in one of the later cells, these below lists will be used as default.
"""

#@title Operators Varations
arithmetic_operators = ["+","-","*","/","^"]
login_comp_operators = ["<","<=",">", ">=", "==", "!="]
cross_sectional_operators = ["rank","normalize","zscore","winsorize","scale_down","scale","generalized_rank","rank_by_side","quantile","one_side"]
symbols = ["(",")","=",","]
group_operators = ["group_count", "group_max", "group_median", "group_min", "group_neutralize", "group_normalize","group_percentage", "group_rank", "group_scale", "group_std_dev","grouop_sum","group_zscore"]
vector_operators = ["vec_avg", "vec_count","vec_ir","vec_kurtosis","vec_max","vec_min","vec_norm","vec_percentage","vec_powersum", "vec_range", "vec_skewness","vec_stddev","vec_sum"]
time_series_operators = ["ts_weighted_delay","hump","hump_decay","inst_tvr","jump_decay","last_diff_value","ts_arg_max","ts_arg_min", "ts_av_diff","ts_count_nans","ts_decay_exp_window", "ts_decay_linear","ts_delay", "ts_delta", "ts_ir","ts_kurtosis","ts_max","ts_max_diff","ts_mean","ts_median","ts_min","ts_min_diff",
                        "ts_min_max_cps","ts_min_max_diff","ts_product","ts_percentage","ts_rank","ts_returns","ts_scale","ts_skewness","ts_std_dev", "ts_sum", "ts_zscore","ts_entropy", "ts_quantile"]

"""The below cell takes up the alpha idea on which the variations will be the running"""

#@title Alpha Idea
"""
vhat=ts_regression(volume,ts_delay(snt_buzz_ret,1),120);
ehat=ts_regression(returns,vhat,120);
alpha=group_neutralize(-ehat*ts_rank(volume,5),bucket(rank(cap),range='0,1,0.1'));
beta = trade_when(abs(returns)<0.075,alpha,abs(returns)>0.10);
"""
sample_alpha = """vhat = ts_regression ( volume , ts_delay ( snt_buzz_ret , 1 ) , 120 ) ; ehat = ts_regression ( returns , vhat , 120 ) ; alpha = group_neutralize ( - ehat * ts_rank ( volume , 5 ) , bucket ( rank ( cap ) , range = '0,1,0.1' ) ) ; trade_when ( abs ( returns ) < 0.075 , alpha , abs ( returns ) > 0.10 )"""

"""The below cell helps the user to assign variable segments of the alpha and allocate binds/custom variations."""

#@title Get Events of Alpha
temp = sample_alpha.split(" ")
ind = 0
for i in temp:
    if '(' not in i and ')' not in i:
      print(ind, i)
    ind+=1

"""The below cell faciliates the variations of the ideated alpha.

*   **not_fixed list** contains the event indices which are going to variate. The rest event indices will be marked as "FIXED" and won't be changed in the whole population.

*   **Bind_dic** keys are "binded" to its value. Eg "10":"5" means the event at index 10 is binded to the event at index 5, which basically means that whatever value index 5 will have, index 10 will contain the same value in the whole population.

*   **datafields_list** will contain the datafields the user want to variate over at the non-fixed event indices which represents a datafield.

*   **numericals** list will contain the values the user want to iterate over at any non-fixed event indices which represents a numerical.

*   **groups** list will contain the all the groups that the user wants to variate in the alpha wherever groups are appearing and are non-fixed.

*   **custom_bind_variations** dictionary helps the user to set a custom list to variate over for any non-fixed event index. This is helpful if the user wants to assign only a certain limited number of options for a variable and not let it iterate over all general possibilites
"""

#@title Bind and Fixed Events
not_fixed = [6, 13, 34]
bind_dic = {
    "24" : "13"
}
datafields_list = ["volume","adv20","high","low","open","close","dividend"]
numericals = [60,120,240]
groups = ["country" ,"exchange", "industry", "market", "sector","subindustry"]
custom_bind_variations = {
    "6" : ["ts_delay", "ts_delta"],
    "34" : ["ts_rank", "ts_zscore"]
}

"""The below cell generates all the variations using the information feeded in the above cells. It also considers the various settings variations."""

#@title Generating different variations
binded_dict = {}

#get_variations takes event_index as input and the returns all the possible variations of the event at that index as a list.
def get_variations(event_index):
    index = index_of[event_index]
    if str(event_index) in bind_dic.keys():
        temp = [binded_dict[str(bind_dic[str(event_index)])],]
        return temp

    elif events[index][0] == "FIXED_OPERATOR" or events[index][0] == "FIXED":
        temp = [events[index][1],]
        return temp

    elif str(event_index) in custom_bind_variations.keys():
        return custom_bind_variations[str(event_index)]

    elif events[index][0] == 'CS_OPERATOR':
        return cross_sectional_operators

    elif events[index][0] == 'GROUP_OPERATOR':
        return group_operators

    elif events[index][0] == 'VEC_OPERATOR':
        return vector_operators

    elif events[index][0] == 'TS_OPERATOR':
        return time_series_operators

    elif events[index][0] == "SYMBOL":
        temp = [events[index][1],]
        return temp

    elif events[index][0] == "ARITHMETIC":
        return arithmetic_operators

    elif events[index][0] == "NUMBER":
        return numericals

    elif events[index][0] == "GROUP":
        return groups

    elif events[index][0] == "DATASET":
        return datafields_list
"""
alpha_gen is a recursive function which helps in generating all the alphas of the population.
Event_index iterates over all the even indices one by one.
Index basically is the starting index of the event in the ideated alpha.
Alpha is the string getting generated which is completed at the end, when index becomes equal to the length of the sample (ideated) alpha
"""
def alpha_gen(event_index, index, alpha):
    if index>=len(sample_alpha):
        alpha_list.append(alpha)
        return

    new_index = index+len(events[index][1])+1
    new_event_index = event_index+1

    variations = get_variations(event_index)
    for x in variations:
        binded_dict[str(event_index)] = x
        alpha_gen(new_event_index, new_index, alpha + ' ' + str(x))

"""
break_exp function takes an alpha as input as string.
It breaks the alpha to a list of events and idenitifes each event as an operator or datafield or number or symbol.
This helps in variating over the list of concerned type to generate the whole populations.
"""

def break_exp(s):
    temp = s.split(" ")
    cur_events = {}
    ind = 0
    index = -1
    index_of = {}
    for i in temp:
      index+=1
      if index not in not_fixed:
          cur_events[ind] = ["FIXED",i]

      elif index<len(temp)-1 and temp[index+1] == '(':
          if i in cross_sectional_operators:
              cur_events[ind] = ["CS_OPERATOR", i]
          elif i in group_operators:
              cur_events[ind] = ["GROUP_OPERATOR", i]
          elif i in vector_operators:
              cur_events[ind] = ["VEC_OPERATOR", i]
          elif i in time_series_operators:
              cur_events[ind] = ["TS_OPERATOR", i]
          else:
              cur_events[ind] = ["FIXED_OPERATOR", i]

      elif index<len(temp)-1 and temp[index+1] == '=':
          cur_events[ind] = ["FIXED",i]

      elif i in arithmetic_operators:
        cur_events[ind] = ["ARITHMETIC", i]
      elif i in groups:
        cur_events[ind] = ["GROUP", i]
      elif i in symbols:
        cur_events[ind] = ["SYMBOL", i]
      elif i.replace(".", "").isnumeric():
        cur_events[ind]= ["NUMBER", i]
      else:
        cur_events[ind] = ["DATASET", i]
      index_of[index] = ind
      ind += len(i)+1
    return cur_events, index_of

events, index_of = break_exp(sample_alpha)
empty_str = ""
alpha_list = []
alpha_gen(0,int(0),empty_str)
df = pd.DataFrame(alpha_list)
alpha_gen_list = []
ndu = []
#the below segment generates alphas with different settings. Sample alpha can be extended to various settings, depending on the information feeded above.
for i in n_list:
  for j in d_list:
    for k in u_list:
        alpha_gen_list.extend([generate_alpha(x,decay=j,universe=k,neutralization=i) for x in alpha_list])
        ndu.append([i, j, k])

#@title Search Space
"""
User can have a look at the search space population by running the code below.
It also prints the size of the whole population
"""
# for i in alpha_list:
#   print(i)
print(len(alpha_gen_list))

"""### **4.1) GRID SEARCH**

Grid Search involves defining a grid of hyperparameters for a given alpha and exhaustively searching through the grid to find the optimal combination of hyperparameters.

After generating all possible alphas, the whole population is simulated and evaluated. Alphas that have atmost 2 fail cases, are considered in the final dataframe. These alphas are then proceeded to check for prod correlation. The final result is stored in the .xlsx file.
"""

#@title Batch Simulations
"""
We run batch simulations of 10 alphas.
stats_list_result contains the results of all batches combined.
The user will be able to view shortlisted alphas and their stats for batches of 10 alphas each.
"""
def grid_search():
  stats_list_result = []
  for i in range(0,len(alpha_gen_list),10):
      batch_list = alpha_gen_list[i:i+10]
      batch_list_result = simulate_alpha_list_multi(s, batch_list, limit_of_multi_simulations=10)
      stats_list_result.extend(batch_list_result)
      display(prettify_result_mod(batch_list_result, clickable_alpha_id=True))
  return stats_list_result

#@title Final Simulated DataFrame
"""
finaldf contains the final shortlisted alpha and their stats
"""
def display_final_df(stats_list_result):
  stats_list_result2 = pd.DataFrame(stats_list_result).drop_duplicates(subset=["alpha_id"]).reset_index().drop("index",axis=1)
  stats_list_result3 = []
  for i in range(len(stats_list_result2)):
    final = {"alpha_id": stats_list_result2["alpha_id"][i],"simulate_data":stats_list_result2["simulate_data"][i],	"is_stats":stats_list_result2["is_stats"][i],	"pnl":stats_list_result2["pnl"][i],
            "stats":stats_list_result2["stats"][i],	"is_tests":stats_list_result2["is_tests"][i]}
    stats_list_result3.append(final)
  finaldf = prettify_result_mod(stats_list_result3,clickable_alpha_id=True)
  display(finaldf)
  return finaldf

#@title Check Prod Correlation
"""
corr_df contains the prod correlation pass/fail status of the shortlisted alphas.
"""
def display_prod_correlation(finaldf):
  corr_ls = []
  finaldf= finaldf.data
  for curr_id in finaldf.loc[:,"alpha_id"]:
      corr_ls.extend(check_prod_corr_test(s,curr_id))
  corr_df = pd.DataFrame(corr_ls)
  corr_df = corr_df.style.format({"alpha_id": make_clickable_alpha_id})
  corr_df = corr_df.data
  finaldf = pd.merge(finaldf, corr_df, on="alpha_id")
  display(corr_df)
  return finaldf

#@title Final Excel
def download_excel(finaldf):
  finaldf.to_excel('search_results.xlsx')

#@title Example Run
"""
We have taken a small batch of variations to facilitate an example run
"""
stats_list_result = grid_search()
finaldf = display_final_df(stats_list_result)
finaldf = display_prod_correlation(finaldf)
download_excel(finaldf)

"""### **4.2) RANDOM SEARCH**

Random Search is another technique used for hyperparameter optimization in alpha based research. Similar to Grid Search, Random Search aims to find the best set of hyperparameters for an alpha, but it does so by randomly sampling the hyperparameter space instead of exhaustively searching through predefined combinations.

After generating all possible alphas, the whole population is randomly sampled and then simulated and evaluated. Alphas that have atmost 2 fail cases, are considered in the final dataframe. These alphas are then proceeded to check for prod correlation. The final result is stored in the .xlsx file. The size of the sample depends on the user.
"""

#@title Random Search
import random
def random_search(x, percent_flag, alpha_gen_list):
  stats_list_result = []
  count = x
  if percent_flag:
    count = int((x*len(alpha_gen_list))/100)

  alpha_gen_list_random = random.sample(alpha_gen_list,count)

  for i in range(0,len(alpha_gen_list_random),10):
      batch_list = alpha_gen_list_random[i:i+10]
      try:
        batch_list_result = simulate_alpha_list_multi(s, batch_list, limit_of_multi_simulations=10)
        stats_list_result.extend(batch_list_result)
        display(prettify_result_mod(batch_list_result, clickable_alpha_id=True))
      except:
        print("Some Exception occured. This batch is skipped")
  return stats_list_result

#@title Example Run
"""
We have taken a large batch of variations to facilitate an example run by adding more variations below
"""
#@title Bind and Fixed Events
not_fixed = [6, 13, 34]
bind_dic = {
    "24" : "13"
}
numericals = [20,40,60,120,240,512]
custom_bind_variations = {
    "6" : ["ts_delay", "ts_delta"],
    "34" : ["ts_rank", "ts_zscore","ts_std_dev"]
}
u_list = ["TOP3000","TOP1000"]

binded_dict = {}
events, index_of = break_exp(sample_alpha)
empty_str = ""
alpha_list = []
alpha_gen(0,int(0),empty_str)
df = pd.DataFrame(alpha_list)
alpha_gen_list = []
ndu = []
#the below segment generates alphas with different settings. Sample alpha can be extended to various settings, depending on the information feeded above.
for i in n_list:
  for j in d_list:
    for k in u_list:
        alpha_gen_list.extend([generate_alpha(x,decay=j,universe=k,neutralization=i) for x in alpha_list])
        ndu.append([i, j, k])

print("Length = ",len(alpha_gen_list))
random_list_result = random_search(24,0,alpha_gen_list)
random_finaldf = display_final_df(random_list_result)
random_finaldf = display_prod_correlation(random_finaldf)
download_excel(random_finaldf)

"""###**4.3 RANDOM SEARCH WITH PRUNING**

Random searching with pruning combines the concepts of random search and pruning techniques to efficiently explore the hyperparameter space and improve the search process.

This algorithm integrates the random sampling of hyperparameters with strategies for reducing the exploration in alphas deemed less promising. It will evaluate the performance of certain hyperparameter combinations for a limited number of iterations and then decide to prune further exploration if the performance is below a certain threshold.

We shuffle the population before going forward with the search to introduce more randomness. We performing pruning after 20%, 40%, 60% and 80% of the limit. The limit of simulation is provided by the user.

For pruning, we find for each non-fixed event, the average fitness of each variation. Then we eliminate those variations which go below one standard deviation value, from further populations. Thus reducing the size of population. Further, we do not prune if the reduced population size goes below the limit. After simulating limit number of alphas, we stop the process and return the shortlisted alphas.
"""

#@title Pruning
import math

index_of = []

"""
Input: index of the next alpha to be simulated, discard flag, fitness cache of each variation and limit of simulations
Returns: Updated discard flag
Prune Function prunes the variations of each non-fixed event index and removes all the alphas containing variations that have performed weakly
with respect to other variations.
"""
def prune(next_alpha_index, discard_flag, variation_fitness_cache, limit, alpha_gen_list):
  for i in not_fixed:
    if discard_flag is False:
      return discard_flag
    variations = get_variations(i)
    n = len(variations)
    avg_fitness = {}
    sum = 0
    crash_flag = False
    if n > 2:
      for j in range(n):
        if not variations[j] in variation_fitness_cache[i]:
          crash_flag = True
          break
        avg_fitness[variations[j]] = variation_fitness_cache[i][variations[j]][0]/variation_fitness_cache[i][variations[j]][1]
        sum+=avg_fitness[variations[j]]

      if crash_flag:
        continue

      mean = sum/n
      sigma = 0

      for j in range(n):
        sigma += ((mean-avg_fitness[variations[j]])**2)
      sigma /= n
      sigma = math.sqrt(sigma)
      new_alpha_gen_list = list(alpha_gen_list)
      for j in range(n):
        if discard_flag is False:
          break
        if avg_fitness[variations[j]] < (mean - (sigma)):
          for alpha in alpha_gen_list[next_alpha_index:]:
            expression = alpha["regular"]
            for m in alpha_list:
              if compare(m,expression):
                m = m.strip()
                new_events = m.split(" ")
                if new_events[i] == variations[j]:
                  new_alpha_gen_list.remove(alpha)
                  if len(new_alpha_gen_list) <= limit:
                    discard_flag = False
                    break
                break

      print("Length before pruning = ",len(alpha_gen_list))
      alpha_gen_list = list(new_alpha_gen_list)
      print("Length after pruning = ",len(new_alpha_gen_list))

  return discard_flag, alpha_gen_list

"""
Input: Limit as an exact value or percentage value
Prints batch wise results of 10 alphas each. Also stores all batches results in the stats_list_result
"""

def pruned_search(limit, percent_flag, alpha_gen_list):
  random.shuffle(alpha_gen_list)
  stats_list_result = []
  if percent_flag:
    limit = int((limit*len(alpha_gen_list))/100)

  #Percentage of limit for pruning -> 20, 40, 60, 80

  next_prune = int(limit/5)
  prune_gap = int(limit/5)
  variation_fitness_cache = {}
  discard_flag = True
  prune_count = 0
  index = -1
  cur_events = sample_alpha.split(" ")
  for event in cur_events:
    index+=1
    variation_fitness_cache[index] = {}

  for i in range(0,limit,10):
      batch_list = alpha_gen_list[i:min(i+10,limit)]
      try:
        batch_list_result = simulate_alpha_list_multi(s, batch_list, limit_of_multi_simulations=10)
        stats_list_result.extend(batch_list_result)
        display(prettify_result_mod(batch_list_result, clickable_alpha_id=True))

        for k in batch_list_result:
          alpha = k['simulate_data']
          sharpeL = list(k['is_stats']['sharpe'])
          sharpe = sharpeL[0]
          returnsL = list(k['is_stats']['returns'])
          returns = returnsL[0]
          turnoverL = list(k['is_stats']['turnover'])
          turnover = turnoverL[0]
          drawdownL = list(k['is_stats']['drawdown'])
          drawdown = drawdownL[0]
          alpha_idL = list(k['is_stats']['alpha_id'])
          alpha_id = alpha_idL[0]
          long_countL = list(k['is_stats']['longCount'])
          long_count = long_countL[0]
          short_countL = list(k['is_stats']['shortCount'])
          short_count = short_countL[0]

          fitness = get_fitness(sharpe, turnover, drawdown, returns, long_count, short_count)
          expression = k['simulate_data']['regular']
          for m in alpha_list:
            if compare(expression,m):
              m = m.strip()
              cur_events = m.split(" ")
              index=-1
              for event in cur_events:
                index+=1
                if not event in variation_fitness_cache[index]:
                    variation_fitness_cache[index][event] = [0,0]
                variation_fitness_cache[index][event][0] += fitness
                variation_fitness_cache[index][event][1] += 1
              break
      except Exception as e:
        print(e)
        print("This batch is skipped")

      if prune_count < 5 and (i+10)>=next_prune and discard_flag:
        prune_count += 1
        discard_flag, alpha_gen_list = prune(i+11,discard_flag, variation_fitness_cache, limit, alpha_gen_list)
        next_prune+=prune_gap
  return stats_list_result

"""This is the example run of the pruned random search.


Since the limit is 24, the count of pruning will be 2 instead of 4, as the batch size is 10.


Please note that there is less number of pruning as the the limit is just 24 and there is not enough data by running prune after 20% of the limit.
Pruning only happens if there has atleast 1 occurence of every variable of an event.
"""

#@title Example Run
not_fixed = [6, 13, 34]
bind_dic = {
    "24" : "13"
}
numericals = [20,40,60,120,240,512]
custom_bind_variations = {
    "6" : ["ts_delay", "ts_delta","ts_rank","ts_arg_max"],
    "34" : ["ts_rank","ts_zscore","ts_std_dev",]
}
u_list = ["TOP3000","TOP1000"]

binded_dict = {}
events, index_of = break_exp(sample_alpha)
empty_str = ""
alpha_list = []
alpha_gen(0,int(0),empty_str)
df = pd.DataFrame(alpha_list)
alpha_gen_list = []
ndu = []
#the below segment generates alphas with different settings. Sample alpha can be extended to various settings, depending on the information feeded above.
for i in n_list:
  for j in d_list:
    for k in u_list:
        alpha_gen_list.extend([generate_alpha(x,decay=j,universe=k,neutralization=i) for x in alpha_list])
        ndu.append([i, j, k])

print("Length = ",len(alpha_gen_list))
pruned_list_result = pruned_search(24,0,alpha_gen_list)

pruned_finaldf = display_final_df(pruned_list_result)
pruned_finaldf = display_prod_correlation(pruned_finaldf)
download_excel(pruned_finaldf)

"""## **5. GPT BASED ALPHA RESEARCH**
This methodology leverages the power of LLMs to generate unique alpha ideas and their corresponding Fast Expression Language codes. For this purpose, we have used GPT due to its evident advantages including ability to learn new programming languages with limited information, ease of prompt engineering and ability to generate unique ideas.

We provide GPT with the idea using which the alphas are to be generated and data fields that we want to generate the alphas on. As the GPT algorithm produces random and different output every time, it requires multiple simulations to generate good alphas. Also, the prompts need to be changed and fine-tuned according to the outputs GPT produces.

GPT has a limit on the number of tokens in the prompts and hence the instructions between system and user prompts have been structured to adjust the token limit while providing necessary context.

After multiple iterations we were able to produce multiple alphas that performed decently and had only 1-2 failing cases which were easy to pass after manual tuning.

**OpenAI API Key**

Note: Enter your own API key before running the cell
"""

#@title OpenAI API Key

MY_API_KEY = ''  # The unique security key for your GPT Account
os.environ['OPENAI_API_KEY'] = MY_API_KEY                           # Setting the environment variable for OpenAI API Key as our unique security key, to be used to make API calls to GPT

"""**System Prompt String**

The prompt to be used to define the role of the system for the given use case, here it is to generate alphas using quantitative research knowledge
"""

#@title System Prompt String

system_prompt_string = """You are a quantitative researcher, skilled in making alphas based on economic and market hypothesis on the WorldQuant Brain platform and implementing them in Fast Expression Programming Language. I will give you a prompt explaining the Fast Expression Language and what you need to do. The prompt has new lines separated by \n.
The final alpha statement in the expression language is the weight given to stocks and these weights are then used to implement a long-short equity trading strategy. Your job is to return list of alpha expressions only.
Use the following idea to make alphas:
"""

"""**User Prompt String**

The base prompt to be given by the user to the system (GPT) which explains the Fast Expression Language and the working of BRAIN to the system.
This prompt introduces Fast Expression to the system so that it can generate alphas through ideas and mathematical formulas which can directly be fed to the BRAIN API for testing.
This base prompt defines and explains all the operators, their expected inputs, outputs and intricacies. It also defines all the evaluation criteria and their mathematical and logical importance
In addition it provides example alphas to the system and explains what output is expected from the system.
"""

#@title User Prompt String
user_prompt_string = """OPERATORS:
Arithmetic Operators
+, -, *, /, ^
Arithmetic operators: add, subtract, multiply, divide, power
<, <=, >, >=, ==, !=
Logic comparison operators: Less Than (or Equal), Greater Than (or Equal), Equal, Not Equal
cond ? expr1 : expr2
If cond is True, then expr1, else expr2. For example: close < open ? close : open
abs(x)
Absolute value of x
add(x, y, filter = false), x + y
Add all inputs (at least 2 inputs required). If filter = true, filter all input NaN to 0 before adding
divide(x, y), x / y
x / y
exp(x)
Natural exponential function: e^x
inverse(x)
1 / x
log(x)
Natural logarithm. For example: Log(high/low) uses the natural logarithm of high/low ratio as stock weights.
log_diff(x)
Returns log(current value of input or x[t] ) - log(previous value of input or x[t-1]). Detailed description
max(x, y, ..)
Maximum value of all inputs. At least 2 inputs are required. Detailed description
min(x, y ..)
Minimum value of all inputs. At least 2 inputs are required. Detailed description
multiply(x ,y, ... , filter=false), x * y
Multiply all inputs. At least 2 inputs are required. Filter sets the NaN values to 1. Detailed description
nan_mask(x, y)
replace input with NAN if input's corresponding mask value or the second input here, is negative. Detailed description
nan_out(x, lower=0, upper=0)
If x < lower or x > upper return NaN, else return x. At least one of "lower", "upper" is required. Detailed description
power(x, y)
x ^ y
replace(x, target="v1 v2 ..vn", dest="d1,d2,..dn");
Replace target values in input with destination values. Detailed description
reverse(x)
•	x
round(x)
Round input to closest integer.
round_down(x, f=1)
Round input to greatest multiple of f less than input;
sign(x)
if input = NaN; return NaN
else if input > 0, return 1
else if input < 0, return -1
else if input = 0, return 0
signed_power(x, y)
x raised to the power of y such that final result preserves sign of x. Detailed description
s_log_1p(x)
Confine function to a shorter range using logarithm such that higher input remains higher and negative input remains negative as an output of resulting function and -1 or 1 is an asymptotic value. Detailed description
sqrt(x)
Square root of x
subtract(x, y, filter=false), x - y
x-y. If filter = true, filter all input NaN to 0 before subtracting
to_nan(x, value=0, reverse=false)
Convert value to NaN or NaN to value if reverse=true
densify(x)
Converts a grouping field of many buckets into lesser number of only available buckets so as to make working with grouping fields computationally efficient. Detailed description.
LOGICAL OPERATORS:
and(input1, input2)
Logical AND operator, returns true if both operands are true and returns false otherwise
or(input1, input2)
Logical OR operator returns true if either or both inputs are true and returns false otherwise
equal(input1, input2), input1 == input2
Returns true if both inputs are same and returns false otherwise
negate(input)
The result is true if the converted operand is false; the result is false if the converted operand is true
less(input1, input2), input1 < input2
If input1 < input2 return true, else return false
if_else(input1, input2, input 3)
If input1 is true then return input2 else return input3.
Time Series Operators:
For time series operator with look back days parameter (d) , d must be <512
days_from_last_change(x)
Amount of days since last change of x
ts_weighted_delay(x, k=0.5)
Instead of replacing today’s value with yesterday’s as in ts_delay(x, 1), it assigns weighted average of today’s and yesterday’s values with weight on today’s value being k and yesterday’s being (1-k). Detailed description
hump(x, hump = 0.01)
Limits amount and magnitude of changes in input (thus reducing turnover).
hump_decay(x, p=0)
This operator helps to ignore the values that changed too little corresponding to previous ones. Detailed description
last_diff_value(x, d)
Returns last x value not equal to current x value from last d days
ts_arg_max(x, d)
Returns the relative index of the min value in the time series for the past d days. If the current day has the min value for the past d days, it returns 0. If previous day has the min value for the past d days, it returns 1. Detailed description
ts_arg_min(x, d)
Returns the relative index of the min value in the time series for the past d days; If the current day has the min value for the past d days, it returns 0; If previous day has the min value for the past d days, it returns 1.
ts_av_diff(x, d)
Returns x - tsmean(x, d), but deals with NaNs carefully. That is NaNs are ignored during mean computation. Detailed description
ts_co_kurtosis(y, x, d)
Returns cokurtosis of y and x for the past d days. Detailed description
ts_corr(x, y, d)
Returns correlation of x and y for the past d days
ts_co_skewness(y, x, d)
Returns coskewness of y and x for the past d days. Detailed description
ts_covariance(y, x, d)
Returns covariance of y and x for the past d days
ts_decay_exp_window(x, d, factor = f)
Returns exponential decay of x with smoothing factor for the past d days. Detailed description
ts_decay_linear(x, d, dense = false)
Returns the linear decay on x for the past d days. Dense parameter=false means operator works in sparse mode and we treat NaN as 0. In dense mode we do not.
ts_delay(x, d)
Returns x value d days ago
ts_delta(x, d)
Returns x - ts_delay(x, d)
ts_ir(x, d)
Return information ratio ts_mean(x, d) / ts_std_dev(x, d)
ts_kurtosis(x, d)
Returns kurtosis of x for the last d days. Detailed description
ts_max(x, d)
Returns max value of x for the past d days
ts_max_diff(x, d)
Returns x - ts_max(x, d)
ts_mean(x, d)
Returns average value of x for the past d days.
ts_median(x, d)
Returns median value of x for the past d days
ts_min(x, d)
Returns min value of x for the past d days
ts_product(x, d)
Returns product of x for the past d days
ts_rank(x, d, constant = 0)
Rank the values of x for each instrument over the past d days, then return the rank of the current value + constant. If not specified, by default, constant = 0.
ts_returns (x, d, mode = 1)
Returns the relative change in the x value .
Detailed description
ts_scale(x, d, constant = 0)
Returns (x – ts_min(x, d)) / (ts_max(x, d) – ts_min(x, d)) + constant
This operator is similar to scale down operator but acts in time series space. Detailed description
ts_skewness(x, d)
Return skewness of x for the past d days. Detailed description
ts_std_dev(x, d)
Returns standard deviation of x for the past d days
ts_step(1), step(1)
Returns days' counter
ts_sum(x, d)
Sum values of x for the past d days.
ts_zscore(x, d)
Z-score is a numerical measurement that describes a value's relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean: (x - tsmean(x,d)) / tsstddev(x,d)
CROSS SECTIONAL OPERATORS
normalize(x, useStd = false, limit = 0.0)
Calculates the mean value of all valid alpha values for a certain date, then subtracts that mean from each element. Detailed description
winsorize(x, std=4)
Winsorizes x to make sure that all values in x are between the lower and upper limits, which are specified as multiple of std. Details can be found on wiki
zscore(x)
Z-score is a numerical measurement that describes a value's relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean
rank_gmean_amean_diff(input1, input2, input3,...)
Operator returns difference of geometric mean and arithmetic mean of cross sectional rank of inputs. Detailed description.
TRANSFORMATIONAL OPERATORS
trade_when(x, y, z)
Used in order to change Alpha values only under a specified condition and to hold Alpha values in other cases. It also allows to close Alpha positions (assign NaN values) under a specified condition. Detailed description
GROUP OPERATORS
group_max(x, group)
Maximum of x for all instruments in the same group.
group_mean(x, weight, group)
All elements in group equals to the mean value of the group. Mean = sum(data*weight) / sum(weight) in each group.
group_median(x, group)
All elements in group equals to the median value of the group.
group_min(x, group)
All elements in group equals to the min value of the group.
group_neutralize(x, group)
Neutralizes Alpha against groups. These groups can be subindustry, industry, sector, country or a constant. Detailed description
group_normalize(x, group, constantCheck=False, tolerance=0.01, scale=1)
Normalizes input such that each group's absolute sum is 1. Detailed description
group_rank(x, group)
Each elements in a group is assigned the corresponding rank in this group
group_sum(x, group)
Sum of x for all instruments in the same group.
group_zscore(x, group)
Calculates group Z-score which is measured in terms of standard deviations from the mean. zscore = (data - mean) / stddev of x for each instrument within its group.
All operator calls are to be made using the following order of arguments:
Required amount of data fields
Group (optional)
Lookback days (optional)
Keyword arguments (optional)
In the “Operator” column, they are shown as key=default_value.
They are passed in the form key=value.
Keyword argument may be omitted, in this case its default value will be passed to operator.
Examples:
ts_delta(close, 21)
ts_regression(close, ts_step(1), 21, lag = 63, rettype = 1)
ts_regression(close, ts_step(1), 21)
same as ts_regression(close, ts_step(1), 21, lag=0, rettype=0)
ts_covariance(close, vwap, 20)
group_rank(close, industry)
group_backfill(sales / assets, subindustry, 252, std = 3)
2.	Variable assignment is possible in the following form:
a = sales / assets; ts_delta(a, 252)
Variable reassignment is not permitted.
The mathematical expression is written in fast expression language.
“Fast expression” is a proprietary programming language used by WorldQuant BRAIN that is designed to make it easier to write and test financial models. The language can be thought as a form of pseudo code, which uses natural language and simple programming constructs to convey the logic of the algorithm.
The goal of using “Fast expression” on BRAIN is to provide a clear and concise way to express complex ideas and algorithms that can be easily understood by other developers and researchers. By abstracting away the details of the underlying implementation, it can allow BRAIN users to focus on the high-level logic of their algorithms, rather than getting bogged down in the implementation details.
Characteristics of Fast Expression
Just like how an English sentence consists of a subject, verb and object; Fast expression can include data fields, operators and numerical values.
Data fields
Data fields refer to a named collection of data, for example 'open price' or 'close price'.
Operators
Operators refer to a set of mathematical techniques required to implement your Alpha ideas.
; (semicolon) acts as a semicolon in a sentence, separating the end of one sentence from the beginning of another sentence. For the last line of the code  ; is not needed.
The last sentence of the entire expression is the alpha expression that the BRAIN simulator use to calculate the positions to take in each stock.
Lastly, Fast expression does not have classes, objects, pointers, or functions.
In-sample simulation uses data over a 10-year timeframe, and tests out how well your alpha performs in the historical period. After the simulation, you will see the IS Summary row with 6 metrics: Sharpe, Turnover, Fitness, Returns, Drawdown, and Margin.
Sharpe
This ratio measures the excess return (or risk premium) per unit of deviation of returns of an Alpha. It takes the mean of the PnL divided by the standard deviation of the PnL. The higher the Sharpe Ratio or Information Ratio (IR), the more consistent the Alpha’s returns are potentially likely to be, and consistency is an ideal trait. The passing requirement for Sharpe on the BRAIN platform is to be above 1.25.
Sharpe = ((252)^0.5)*(Mean(PnL)/Stdev(PnL))
Turnover
Turnover of an Alpha is a metric that measures the simulated daily trading activity, i.e., how often the Alpha trades. It can be defined as the ratio of value traded to book size. The higher the turnover, the more often a trade occurs. Since trading incurs transaction costs, reducing turnover is generally an ideal trait. The passing requirement for turnover on the BRAIN platform is to be between 1% and 70%.
Turnover = DollarTradingValue/Book Size
Fitness
Fitness of an Alpha is a function of Returns, Turnover & Sharpe. Fitness is defined as:
Fitness = Sharpe*(abs(returns)/max(Turnover,0.125)
Good Alphas generally have high fitness. You can seek to improve the performance of your Alphas by increasing Sharpe (or returns) and reducing turnover. The passing requirement for fitness on the BRAIN platform is to be greater than 1.0.
Returns
Returns is the amount made or lost by the Alpha during a defined period and is expressed in percentages. BRAIN defines returns as:
AnnualReturn = AnnualizedPnL/(0.5*Book Size)
Drawdown
Drawdown of an Alpha is the largest reduction in simulated PnL during a given period, expressed as a percentage. It is calculated as follows:
Drawdown = Dollar Amount of Largest Peak To Trough Gap in PnL/(0.5*Booksize)
Margin
Margin is the simulated profit per dollar traded of an Alpha; calculated as:
Margin = PnL/TotalDollars Traded

Here are some example alphas with their hypothesis and implementation in Fast Expression:
1.	Stocks with higher EBIT compared to CapEx can be a sign of the company not investing much in growth and the stock may not grow as much, thus we should sell those stocks. CapEx (capital expenditure) is money spent on acquiring/maintaining fixed assets. Compare CapEx with EBIT, which is a company's net income before interest and taxes, and short stocks with inadequate CapEx.
Implementation: -rank(ebit/capex)
2.	Companies that spend more money on R&D as a proportion of their revenue may outperform others in the future. Companies are ranked based on R&D expense to revenue ratio
Implementation: rank(mdf_rds)
3.	Buy more of stocks with a higher earnings yield rank in the sector. Using group_rank operator, earning yield rank of a stock is compared against other companies within the same sector.
Implementation: group_rank(fam_est_eps_rank, subindustry)
4.	It is usually safer to go long on companies that can easily pay back the short term debt using high liquid assets. Zscore of the ratio between cash and short term debt is calculated with higher readings refer to higher ratio when compared with the market.
Implementation: zscore(cash_st / debt_st)
Based on these create a good alpha in fast expression language,

Now, understand the operators carefully and using these operators perform various operations on the data fields given below to come up with 7 unique alpha ideas and give mathematical equation of the alphas using fast expression language. Use all the operators, especially binary operators(+,-,/,*,^) and time series operators (like ts_zscore, ts_mean etc...):
"""

"""**Output Format Prompt**


*   This prompt is also a part of the system prompt and defines the output format that is expected from the system
*   This enables us to create a pipeline whose output can directly be tested using the BRAIN API and prevents GPT from producing output in random formats that can not be standardized.


"""

#@title Output Format Prompt

format_instruction = """Return answer in the following format:
Do not write anything else in the output except alpha expressions. Do not label or number anything. Do not put any text in the output.
eg: "group_rank(ern1_spe/ern1,revenue,1), group_rank(ern1_spe - ern1_tse_spe,revenue,1)"
"""

"""**Generate Alpha**


*   The below function generates the alpha using GPT
*   It accepts as input the list of data fields that is supposed to be used by GPT and abstract of the idea based on which the alpha should be generated
*   The accepted data type for both inputs is string


"""

#@title Generate Alpha

def generate_ai_alpha(data_fields, abstract):
  client = OpenAI()                                                       # Defining Open AI as the client site
  system_prompt = system_prompt_string + abstract                         # Creating the system prompt which is the concatenation of the base system prompt and the abstract based on which alpha needs to be generated
  user_prompt = user_prompt_string + data_fields + format_instruction     # Creating the user prompt by concatenating the base user prompt, the target data fields and the output format instruction
  completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
  ]
  )                                                                       # API Call to GPT 3.5 Turbo with our system and user prompts
  content = completion.choices[0].message.content                         # Extracting output content from the return value
  alpha_list = content.split('\n')                                        # Splitting the output into separate alphas
  alpha_list = list(filter(lambda item: item!='',alpha_list) )            # Filtering empty strings generated after splitting the output

  n = len(alpha_list)
  for i in range(n):
    print(alpha_list[i])                                                                            # Iterate over all the alphas and make a call to the BRAIN API to simulate results
    alpha_settings = [generate_alpha(alpha_list[i],decay=4,universe="TOP3000",neutralization="NONE")]   # Defining the simulation settings for the alpha
    stats_list_result = simulate_alpha_list(s,alpha_settings)                                           # Simulating the results from IS Testing the alpha
    try:
      df = prettify_result_mod(stats_list_result, clickable_alpha_id=True)                                # Modifying the dataframe to display in a more interactive and user friendly format
      display(df)
    except:
      print("error")
      continue

"""**Input Datafields and Input Hypothesis / Research Paper Text**

*   Data fields that are to be used for generating the alpha. The format is - data field followed by its explanation as defined on the WorldQuant platform
*   Abstract of the idea based on which we want alphas to be generated. It can be anything from summary of a research paper to crux of individual macro-economic research
"""

# Data fields that are to be used for generating the alpha. The format is - data field followed by its explanation as defined by WorldQuant.
# GPT will generally use the datafields that are relevant to the idea abstract that we have given.
# It is a good practice to run multiple iterations of this for a given idea and data fields to get good alphas. This helps navigate the randomness in GPT outputs.

data_fields = """
oth143_sell_industry_concentration : Summation of squared market shares of industry brokers for selling activities
oth143_buy_industry_concentration : Summation of squared market shares of industry brokers for buying activities
anl14_actvalue_eps_fp0 : Estimated earnings per share
anl14_actvalue_epsrep_fp0: Reported earnings per share
mdl77_2400_chg12msip : 12 Month Change in Short Interest Position. It is defined as the 12 month relative change in short interest position (SIP), where SIP is the number of shares sold short versus total shares outstanding.
mdl77_2400_chgqtrepssurp : Change in Real Earnings Surprise : It is defined as the change in quarterly earnings surprise (Actual EPS - Consensus Estimate EPS), scaled by the most recent closing price.
mdl77_2_htwrgspe_amq : Earnings Growth Rank : It is defined as the one quarter change in a stock's trailing twelve month earnings per share, scaled by its recent closing stock price (E/P).
"""

abstract = """
The difference in the market share for selling and buying by industry brokers reflects an imbalance in trading volume concentration between the selling and buying sides of the industry.
This imbalance could be exploited to predict stock returns, suggesting that a greater discrepancy in liquidity concentration between selling and buying might influence stock prices and offer strategic trading opportunities.
"""

generate_ai_alpha(data_fields, abstract)

"""## **6. EVOLUTIONARY ALGORITHM BASED ALPHA RESEARCH**

 **Genetic Representation of Alphas**

Each alpha is encoded as a fixed-length binary string, encompassing both simulation settings and the alpha expression. Various settings, datafields, operators, and constants are assigned specific bits in the string. For example, the initial bits may signify the universe, followed by decay, neutralization, and the remaining bits representing the alpha expression in a similar manner. This encoding ensures a uniform representation where each bit corresponds to a specific element in the overall alpha configuration.

 **Evolutionary Algorithm**

The algorithm employs reproduction, crossover, mutation, and selection mechanisms to generate new offspring. In each regeneration step, the fitness function is utilized to identify the best-fit individuals for reproduction. Through crossover and mutation, new offspring is produced, and the least-fit individuals in the population are replaced with these new individuals, implementing the elimination through competition mechanism.

**Crossover and Mutation**

The crossover function exchanges two parents at a random point, yielding two new offspring. The mutation function randomly chooses a bit in the binary expression and flips its value.

**Fitness function**

The fitness function evaluates alphas based on a high sharpe ratio, favorable returns, low turnover, and minimal drawdown.
$$
Fitness = \frac{sharpe \ . \ returns}{turnover \ . \ drawdown}
$$

**Variations**

The below cell initializes the different variations for datafields, operators, constants and simulation settings.
"""

#@title Variations
datafield_list_ = ["adv20", "close", "high", "low", "open", "returns","volume","vwap"]
d_list_ = [0,4,8,12]
n_list_ = ["MARKET", "SECTOR", "INDUSTRY", "SUBINDUSTRY"]
u_list_ = ["TOP3000","TOP1000","TOP500","TOP200"]
o_list_ = ["rank","zscore","winsorize","normalize","rank_by_side","sigmoid","pasteurize","log"]
o_list_2_ = ["ts_zscore","ts_rank","ts_arg_max","ts_arg_min","ts_backfill","ts_delta","ts_ir","ts_mean","ts_median","ts_product","ts_std_dev"]
time_list_ = [20,40,60,80,120]

#@title Helper Functions

def get_len(ls):
  return math.ceil(math.log2(len(ls)))

def get_index_datafield(datafield):
  for i in range(len(datafield_list_)) :
    if datafield_list_[i] == datafield :
        return i
  return -1

def get_index_d(decay):
  for i in range(len(d_list_)) :
    if d_list_[i] == decay :
        return i
  return -1


def get_index_n(neutralization):
  for i in range(len(n_list_)) :
    if n_list_[i] == neutralization :
        return i
  return -1


def get_index_u(universe):
  for i in range(len(u_list_)) :
    if u_list_[i] == universe :
        return i
  return -1

def get_index_o(operator):
  for i in range(len(o_list_)):
    if o_list_[i] == operator:
      return i
    return -1

def get_index_o_2(operator):
  for i in range(len(o_list_2_)):
    if o_list_2_[i] == operator:
      return i
    return -1

def decimalToBinary(n):
    return bin(n).replace("0b", "")

def binaryToDecimal(n):
    return int(n,2)

def get_binary_string(index, length):
  s1 = decimalToBinary(index)
  s2 = ""
  for i in range(length-len(s1)):
    s2 = s2 + str(0)
  s1 = s2+s1
  return s1

def generate_zero(length):
  zero = ""
  for i in range(length):
    zero += "0"
  return zero

def string_to_list(binary_alpha):
  ls = []
  for i in binary_alpha:
    ls.append(int(i))
  return ls

def list_to_string(ls):
  binary_alpha = ""
  for i in ls:
    binary_alpha += str(i)
  return binary_alpha

"""**Validity Check**


*   The below function checks if the binary expression passed generates a valid alpha expression given the number of nested operators the initial population was made of.
*   Input: Binary string and Number of nested operators
*   Returns True/False


"""

#@title Validity Check

def check_validity(binary_alpha, flag):
  i=0
  j=get_len(u_list_)
  temp = binary_alpha[i:j]
  temp_val = binaryToDecimal(temp)
  if temp_val >= len(u_list_):
    return False

  i=j
  j+= get_len(d_list_)
  temp = binary_alpha[i:j]
  temp_val = binaryToDecimal(temp)
  if temp_val >= len(d_list_):
    return False

  i=j
  j+=get_len(n_list_)
  temp = binary_alpha[i:j]
  temp_val = binaryToDecimal(temp)
  if temp_val >= len(n_list_):
    return False

  flag_stack=[]
  for k in range(flag):
    i=j
    j+=1
    temp = binary_alpha[i:j]
    temp_val = binaryToDecimal(temp)
    two_flag=0
    if temp_val==1:
      two_flag=1
    flag_stack.append(two_flag)

    if two_flag==0:
      i=j
      j+=get_len(o_list_)
      temp = binary_alpha[i:j]
      temp_val = binaryToDecimal(temp)
      if temp_val >= len(o_list_):
        return False

    else:
      i=j
      j+=get_len(o_list_2_)
      temp = binary_alpha[i:j]
      temp_val = binaryToDecimal(temp)
      if temp_val >= len(o_list_2_):
        return False

  i=j
  j+=get_len(datafield_list_)
  temp = binary_alpha[i:j]
  temp_val = binaryToDecimal(temp)
  if temp_val >= len(datafield_list_):
    return False

  for k in range(flag):
    two_flag = flag_stack.pop()
    if two_flag:
      i=j
      j+=get_len(time_list_)
      temp = binary_alpha[i:j]
      temp_val = binaryToDecimal(temp)
      if temp_val >= len(time_list_):
        return False

  return True

"""**Binary expression to Alpha expression**


*   The below function generates the alpha expression from the binary expression given the number of nested operators the initial population was made of.
*   Input: Binary string and number of nested operators
*   Returns the alpha expression


"""

#@title Binary expression to Alpha expression

def binary_to_alpha(binary_alpha,flag):
  if check_validity(binary_alpha,flag) == False:
      return str("")

  alpha = ""
  settings = []
  i=0
  j=get_len(u_list_)
  temp = binary_alpha[i:j]
  temp_val = binaryToDecimal(temp)

  settings.append(u_list_[temp_val])


  i=j
  j+=get_len(d_list_)
  temp = binary_alpha[i:j]
  temp_val = binaryToDecimal(temp)

  settings.append(d_list_[temp_val])


  i=j
  j+=get_len(n_list_)
  temp = binary_alpha[i:j]
  temp_val = binaryToDecimal(temp)

  settings.append(n_list_[temp_val])

  flag_stack=[]
  for k in range(flag):
    i=j
    j+=1
    temp = binary_alpha[i:j]
    temp_val = binaryToDecimal(temp)
    two_flag=0
    if temp_val==1:
      two_flag=1
    flag_stack.append(two_flag)

    i=j
    if two_flag==1:
      j+=get_len(o_list_2_)
      temp = binary_alpha[i:j]
      temp_val = binaryToDecimal(temp)
      alpha += str(o_list_2_[temp_val]) + " ( "
    else:
      j+=get_len(o_list_)
      temp = binary_alpha[i:j]
      temp_val = binaryToDecimal(temp)
      alpha += str(o_list_[temp_val]) + " ( "


  i=j
  j+=get_len(datafield_list_)
  temp = binary_alpha[i:j]
  temp_val = binaryToDecimal(temp)

  alpha += datafield_list_[temp_val] + " "

  for k in range(flag):
    two_flag = flag_stack.pop()
    if two_flag==1:
      i=j
      j+=get_len(time_list_)
      temp = binary_alpha[i:j]
      temp_val = binaryToDecimal(temp)

      alpha += ", " + str(time_list_[temp_val]) + " "

    alpha += str(") ")
  return alpha,settings

"""**Generate Population**


*   The below function generates the initial population
*   Input: Population size and number of nested operators
*   Returns the population list


"""

#@title Generate Population
def create_individual(binarylen):
    individual=[]
    for i in range(binarylen):
        j= random.randint(0,1)
        individual.append(random.randint(0,1))
    return individual

def generate_pop(pop_size,nested_flag):
  n = get_len(u_list_)+get_len(d_list_)+get_len(n_list_)+ (1 + get_len(o_list_)+get_len(datafield_list_)+get_len(time_list_))*(nested_flag)
  pop=0
  population=[]
  while pop<pop_size:
    individual = create_individual(n)
    if check_validity(list_to_string(individual),nested_flag):
        pop+=1
        population.append(individual)

  return population

"""**Mutation**


*   The below function flips a random bit in the binary string with a given probability
*   Input: Binary string, probability, number of nested operators
*   Returns the new binary string


"""

#@title Mutation
def mutation(individual,probability,nested_flag):
    n=len(individual)
    check=random.uniform(0,1)
    prev_individual = list(individual)
    if check<=probability:
        position=random.randint(0,n-1)
        individual[position]=1-individual[position]
    if check_validity(list_to_string(individual),nested_flag) == False:
      individual = prev_individual
    return individual

"""**Crossover**


*   The below function exchanges two parents' cross-section at a random point, generating two new offsprings
*   Input: Parent1, Parent2, number of nested operators
*   Returns two new binary individuals


"""

#@title Crossover
def crossover(parent1,parent2,nested_flag):
    n=len(parent1)
    position=random.randint(1,n-2)
    child1=[]
    child2=[]
    for i in range(position+1):
        child1.append(parent1[i])
        child2.append(parent2[i])
    for i in range(position+1,n):
        child1.append(parent2[i])
        child2.append(parent1[i])
    if check_validity(list_to_string(child1),nested_flag) == False:
      child1 = parent1
    if check_validity(list_to_string(child2),nested_flag) == False:
      child2 = parent2
    return child1,child2

"""**Tournament Selection**


*   The below function generates the new population by randomly shuffling the population and selecting the fittest individuals
*   Input: Population, Fitness of each individual, Population size
*   Returns the new population


"""

#@title Tournament Selection
def tournament_selection(population, fitnesses, pop_size):
    new_population=[]
    for j in range(2):
        zipped = list(zip(population, fitnesses))
        random.shuffle(zipped)
        population, fitnesses = zip(*zipped)
        for i in range(0,pop_size-1,2):
            if fitnesses[i] > fitnesses[i+1]:
                new_population.append(population[i])
            else:
                new_population.append(population[i+1])
    random.shuffle(new_population)
    return new_population

"""**Compute Fitness**


*   The below function converts the binary individuals in the population to alpha expression, simulates the alphas and computes the fitness of each individual from the simulation stats.
*   Input: Population, number of nested operators
*   Returns the fitness list


"""

#@title Compute Fitness

def pre_compute_fitness(ls, nested_flag):
    fitnesses = []

    alpha_population = []
    settings_population = []
    for i in ls:
      alpha, settings = binary_to_alpha(list_to_string(i),nested_flag)
      alpha_population.append([alpha,settings])

    alpha_gen_list = [generate_alpha(x,decay=y[1],universe=y[0],neutralization=y[2]) for x,y in alpha_population]
    result = simulate_alpha_list_multi(s, alpha_gen_list, limit_of_multi_simulations=10)

    list_of_is_stats = [
        result[x]["is_stats"]
        for x in range(len(result))
        if result[x]["is_stats"] is not None
    ]

    for i in list_of_is_stats:
      sharpe = i.loc[:,'sharpe'][0]
      returns = i.loc[:,'returns'][0]
      turnover = i.loc[:,'turnover'][0]
      drawdown = i.loc[:,'drawdown'][0]
      if sharpe and returns:
        fitnesses.append((sharpe*returns)/(turnover*drawdown))
      else:
        fitnesses.append(-1 * (abs(sharpe)*abs(returns))/(turnover*drawdown))

    for i in range(len(alpha_population)):
        print(i,alpha_population[i])
        print("Fitness: ",fitnesses[i])

    return fitnesses

"""**Genetic Algorithm**


*   The below function develops new individuals for a fixed number of generations using the crossover, mutation and tournament selection mechanisms
*   Input: Number of nested operators, Population size, Number of generations
*   Prints the best-fit individual of each generation


"""

#@title Genetic Algorithm

def genetic_algo(nested_flag,pop_size,generations):
  n = get_len(u_list_)+get_len(d_list_)+get_len(n_list_)+ (1 + get_len(o_list_)+get_len(datafield_list_)+get_len(time_list_))*(nested_flag)
  gen=0
  population = generate_pop(pop_size,nested_flag)
  while(gen!=generations):
    best_fitness=-1e9
    fittest_individual=population[0]
    fitnesses = pre_compute_fitness(population,nested_flag)

    index=-1
    for individual in population:
        index+=1
        f=fitnesses[index]
        if f>best_fitness:
            best_fitness=f
            fittest_individual=individual

    print("Generation: ",gen,"Best Fitness: ",best_fitness,"Individual: ",fittest_individual, "Alpha: ", binary_to_alpha(list_to_string(fittest_individual),nested_flag))
    gen+=1
    population=tournament_selection(population, fitnesses,pop_size)
    new_population=[]
    for i in range(0,pop_size-1,2):
        child1,child2=crossover(population[i],population[i+1],nested_flag)
        new_population.append(child1)
        new_population.append(child2)
    for individual in new_population:
        copy_indiv = list(individual)
        if(gen<math.ceil(0.9*pop_size)):
            individual=mutation(copy_indiv,0.7,nested_flag)
        else:
            individual=mutation(copy_indiv,0.2,nested_flag)
    population=new_population

#Input Number of nested operators, Population size, Number of generations
genetic_algo(3, 20, 5)

"""## **7. HIERARCHICAL EVOLUTIONARY ALGORITHM**

 **Genetic Representation of Alphas**

Each alpha is represented as a hierarchical formulaic tree, where leaves signify datafields or constants, and each node represents an operator. The number of children for each node corresponds to the parameters the operator takes as arguments. For instance, a depth-1 tree signifies a basic alpha with a single operator, while higher-depth trees portray more intricate alphas with nested operators in a hierarchical structure.

 **Hierarchical Search Strategy**

 Recognizing a pattern in alpha effectiveness, higher-depth alphas incorporating successful lower-depth alphas demonstrate increased effectiveness. To foster diversity and effectiveness, our population maintains a collection of root alphas. The crossover operation strategically explores nearby effective alphas of lower depth, enhancing efficiency in acquiring diverse and effective alphas. To establish an optimal gene pool, we iteratively use a hierarchical structure to generate alphas from lower to higher depths.

**Population Generation**

The initial alpha population is randomly generated from lists of datafields, operators, and constants. Population size is determined by the number of unique datafields in the search space. For greater depths, the initial population is created by combining the best-fit alphas from the immediately lower depth.

**Crossover and Mutation**

The crossover function swaps valid subtrees between selected parents, ensuring compatibility with generated alphas. The mutation function randomly selects a node in the tree and replaces its data, operators, or constants with corresponding elements from the set of possibilities.

**Fitness function**

The fitness function evaluates alphas based on a high sharpe ratio, favorable returns, low turnover, and minimal drawdown.
$$
Fitness = \frac{sharpe^2 \ . \ returns}{(turnover+1) \ . \ drawdown}
$$
"""

#@title Variations

data_fields = ["close","open","high","low","vwap","adv20","volume","cap","returns","dividend"] + get_datafields_from_dataset("Technical and Fundamental ranking model","MATRIX")[:20]
unary_ops = ["rank","zscore","winsorize","normalize","rank_by_side","sigmoid","pasteurize","log"]

ts_ops = ["ts_zscore","ts_rank","ts_arg_max","ts_arg_min","ts_backfill","ts_delta","ts_ir","ts_mean","ts_median","ts_product","ts_std_dev"]
ts_ops_flag = ["20","40","60","120","240"]

grp_ops = ["group_median","group_max","group_min","group_neutralize","group_normalize","group_rank","group_sum","group_zscore","group_std_dev"]
grp_ops_flag = ["SUBINDUSTRY","INDUSTRY","MARKET","SECTOR","COUNTRY"]

binary_ops = ["add","subtract","divide","multiply","max","min"]

hump_op = ["hump",]
hump_op_flag = ["hump=0.001","hump=0.002","hump=0.003","hump=0.004"]

truncate_op = ["truncate",]
truncate_op_flag = ["maxpercent=0.1",]

trade_when_op = ["trade_when",]
trade_when_op_flag = ["volume>ts_mean(volume,20)","rank(ts_std_dev(returns,22))>0.5"]

outside_ops = ["ts_backfill", "normalize", "winsorize", "truncate", "hump", "trade_when"]

fixed_alphas = []

"""**Alpha-Tree Definition**


*   A class for the tree node has been created. There are functions defined below that convert alpha expression to tree and tree to alpha expressions
*   tree_to_alpha takes root of the tree as input and returns the alpha expression
*   alpha_to_tree takes the alpha expression as input and returns the root of the tree


"""

#@title Alpha-Tree Definition

class node:
    def __init__(self, data):
      self.data = data
      self.left = self.right = None
      self.fixed = None
      self.flag = 0

    def insert(self, left, right):
      if left == "" :
        self.left = None
      else:
        self.left = node(left)
      if right == "" :
        self.right = None
      else:
        self.right = node(right)

    def set_flag(self, fixed):
      self.fixed = fixed
      self.flag=1


    def PrintTree(self):
      if self.left:
         self.left.PrintTree()
      print( self.data),
      if self.right:
         self.right.PrintTree()

    def cloning(self, n) :
      if (n != None) :
        point = node(n.data)
        point.flag=n.flag
        point.fixed=n.fixed
        point.left = self.cloning(n.left)
        point.right = self.cloning(n.right)
        return point

      return None

    def cloneTree(self) :
      tree = self.cloning(self)
      return tree


def tree_to_alpha(root):
    alpha = ""
    if root.data in unary_ops:
      if not root.flag:
        alpha+= root.data + " ( " + tree_to_alpha(root.left) + " )"
        return alpha
      else:
        alpha+= root.data + " ( " + tree_to_alpha(root.left) + " , " + root.fixed + " )"
        return alpha

    elif root.data in ts_ops or root.data in grp_ops or root.data in truncate_op or root.data in hump_op:
      alpha+= root.data + " ( " + tree_to_alpha(root.left) + " , " + root.fixed + " )"
      return alpha

    elif root.data in trade_when_op:
      alpha+= root.data + " ( " + root.fixed + " , " + tree_to_alpha(root.left) + " , " + " -1 " + " )"
      return alpha

    elif root.data in binary_ops:
      alpha += root.data + " ( " + tree_to_alpha(root.left) + " , " + tree_to_alpha(root.right) + " )"
      return alpha

    else:
      alpha += root.data
      return alpha

def alpha_to_tree(alpha):
    split_list = alpha.split(" ")
    tree,_ = recursive_AtoT(split_list,0)
    return tree


def recursive_AtoT(split_list, index):
    if split_list[index]=="," or split_list[index]=="(" or split_list[index]==")":
      return recursive_AtoT(split_list, index+1)

    elif split_list[index] in unary_ops:
      tree = node(str(split_list[index]))
      tree.left, left_ind = recursive_AtoT(split_list, index+1)
      return tree,left_ind

    elif split_list[index] in binary_ops :
      tree = node(str(split_list[index]))
      tree.left, left_ind = recursive_AtoT(split_list, index+1)
      tree.right, right_ind = recursive_AtoT(split_list, left_ind+1)
      return tree, right_ind

    elif split_list[index] in ts_ops or split_list[index] in grp_ops:
      tree = node(str(split_list[index]))
      tree.left, left_ind = recursive_AtoT(split_list, index+1)
      tree.fixed, right_ind = recursive_AtoT(split_list, left_ind+1)
      return tree, right_ind

    elif split_list[index] in ts_ops_flag or split_list[index] in grp_ops_flag:
      return str(split_list[index]),index

    else:
      return node(str(split_list[index])),index

"""**Crossover**


*   The below function exchanges the subtrees of the two parent trees to generate the new offsprings
*   Input: Parent tree 1, parent tree 2
*   Returns two new trees


"""

#@title Crossover
def crossover(tree1, tree2):
    child1 = tree1.cloneTree()
    child2 = tree2.cloneTree()
    ch1 = random.randint(0,1)
    ch2 = random.randint(0,1)

    if child1.left is None or child2.left is None:
      return child1, child2

    flag=0
    temp = child1.left
    if ch1==1 and tree1.right is not None:
      temp = child1.right
      flag=1

    if ch2==1 and tree2.right is not None:
      if flag:
        child1.right = child2.right
      else:
        child1.left = child2.right
      child2.right = temp

    else:
      if flag:
        child1.right = child2.left
      else:
        child1.left = child2.left
      child2.left = temp

    return child1, child2

"""**Mutation**


*   The below function counts the number of nodes, randomly selects one of them and is changed according to its type with the given probability
*   Input: the root of the tree, probability
*   Returns the new tree


"""

#@title Mutation

def dfs(root):
    if root:
      count = 1 + dfs(root.left) + dfs(root.right)
      return count
    else:
      return 0

def bfs(root,position):
    queue = []
    queue.append(root)
    cnt=0
    while(len(queue) > 0):
      node = queue.pop(0)
      cnt+=1
      if cnt== position:
        return node
      if node.left is not None:
            queue.append(node.left)

      if node.right is not None:
          queue.append(node.right)
    return None

u_ops = unary_ops + ts_ops + grp_ops + trade_when_op + truncate_op + hump_op
def mutation(tree, probability):
    new_tree = tree.cloneTree()
    check=random.uniform(0,1)
    if check<=probability :
      number_of_nodes = dfs(new_tree)
      position=random.randint(1,number_of_nodes)
      node = bfs(new_tree, position)
      if node:
        if node.data in u_ops:
          node.flag = 0
          node.fixed = None
          index = random.randint(0,len(u_ops)-1)
          node.data = u_ops[index]
          if u_ops[index] in ts_ops:
            node.flag=1
            index2 = random.randint(0,len(ts_ops_flag)-1)
            node.fixed = ts_ops_flag[index2]
          elif u_ops[index] in grp_ops:
            node.flag=1
            index2 = random.randint(0,len(grp_ops_flag)-1)
            node.fixed = grp_ops_flag[index2]
          elif u_ops[index] in trade_when_op:
            node.flag=1
            index2 = random.randint(0,len(trade_when_op_flag)-1)
            node.fixed = trade_when_op_flag[index2]
          elif u_ops[index] in truncate_op:
            node.flag=1
            index2 = random.randint(0,len(truncate_op_flag)-1)
            node.fixed = truncate_op_flag[index2]
          elif u_ops[index] in hump_op:
            node.flag=1
            index2 = random.randint(0,len(hump_op_flag)-1)
            node.fixed = hump_op_flag[index2]

        elif node.data in binary_ops:
          index = random.randint(0,len(binary_ops)-1)
          node.data = binary_ops[index]

        else:
          temp = data_fields + fixed_alphas
          index = random.randint(0,len(temp)-1)
          node.data = temp[index]

    return new_tree

"""**Initial Population Creation**


*   The below function generates the initial population
*   Input: Population size
*   Returns the list of trees as well as alphas
"""

#@title Initial Population Creation
fitness_cache = {}
def pick_op():
  b_ops = binary_ops
  ch = random.uniform(0,1)
  if ch<0.7:
    op_index = random.randint(0,len(u_ops)-1)
    return u_ops[op_index]
  else:
    op_index = random.randint(0,len(b_ops)-1)
    return b_ops[op_index]

def create_individual():
  all_ops = u_ops

  op_index = random.randint(0,len(all_ops)-1)
  selected_op = pick_op()
  individual = node(selected_op)

  i = random.randint(0,len(data_fields)-1)
  selected_df = data_fields[i]

  ch = random.uniform(0,1)

  if ch>0.3 and len(fixed_alphas)>0:
    j = random.randint(0,len(fixed_alphas)-1)
    selected_df = fixed_alphas[j]


  if selected_op in unary_ops:
    individual.left = node(selected_df)
    return individual

  elif selected_op in ts_ops:
    individual.left = node(selected_df)
    ts_flag_index = random.randint(0,len(ts_ops_flag)-1)
    individual.set_flag(ts_ops_flag[ts_flag_index])
    return individual

  elif selected_op in grp_ops:
    individual.left = node(selected_df)
    grp_flag_index = random.randint(0,len(grp_ops_flag)-1)
    individual.set_flag(grp_ops_flag[grp_flag_index])
    return individual

  elif selected_op in trade_when_op:
    individual.left = node(selected_df)
    trade_when_flag_index = random.randint(0,len(trade_when_op_flag)-1)
    individual.set_flag(trade_when_op_flag[trade_when_flag_index])
    return individual

  elif selected_op in truncate_op:
    individual.left = node(selected_df)
    truncate_flag_index = random.randint(0,len(truncate_op_flag)-1)
    individual.set_flag(truncate_op_flag[truncate_flag_index])
    return individual

  elif selected_op in hump_op:
    individual.left = node(selected_df)
    hump_flag_index = random.randint(0,len(hump_op_flag)-1)
    individual.set_flag(hump_op_flag[hump_flag_index])
    return individual

  else:
    i2 = random.randint(0,len(data_fields)-1)
    selected_df2 = data_fields[i2]

    ch = random.uniform(0,1)
    if ch>0.5 and len(fixed_alphas)>0:
      j = random.randint(0,len(fixed_alphas)-1)
      selected_df2 = fixed_alphas[j]

    individual.left = node(selected_df)
    individual.right = node(selected_df2)
    return individual


def generate_pop(pop_size):
  tree_pop=[]
  alpha_pop=[]
  pop = 0
  while pop<pop_size:
    individual = create_individual()
    tree_pop.append(individual)
    alpha_pop.append(tree_to_alpha(individual))
    pop+=1
  return tree_pop, alpha_pop

"""**Compute Fitness**



*   The below function simulates the alpha population, extracts the results and computes the fitness for each alpha
*   Input: Alpha population, Current generation, Total number of generations and population size
*   Returns the list of fitness of each indiviual in the population


"""

#@title Compute Fitness

def simplify(s):
  s2 = str(s)
  s2 = s2.replace(" ","")
  s2 = s2.lower()
  return s2

def compare(s1, s2):
    s3 = str(s1)
    s4 = str(s2)
    s3 = s3.replace(" ","")
    s4 = s4.replace(" ","")
    if s3.casefold()==s4.casefold():
        return True
    return False

def compute_fitness(alpha_pop, gen, gen_size, pop_size):
    pop_fitness = []

    short_alpha_pop = []
    old_alpha_pop = []
    old_pop_fitness = []

    for i in alpha_pop:
      if simplify(i) in fitness_cache.keys():
        old_alpha_pop.append(i)
        old_pop_fitness.append(fitness_cache[simplify(i)])
      else:
        short_alpha_pop.append(i)

    new_pop_fitness = []
    for i in range(pop_size):
      new_pop_fitness.append(-1e9)

    if len(short_alpha_pop)>0:
          alpha_gen_list = [generate_alpha(x,decay=4,neutralization="SUBINDUSTRY") for x in short_alpha_pop]

          result = simulate_alpha_list_multi(s, alpha_gen_list, limit_of_multi_simulations=10)

          if gen==gen_size :
            display(prettify_result_mod(result, clickable_alpha_id = True))

          list_of_is_stats = [
              result[x]["is_stats"]
              for x in range(len(result))
              if result[x]["is_stats"] is not None
          ]

          expressions = {
              result[x]["alpha_id"]: result[x]["simulate_data"]["regular"]
              for x in range(len(result))
              if result[x]["is_stats"] is not None
          }


          new_alpha_pop = []
          index=0
          for i in list_of_is_stats:
            sharpe = i.loc[:,'sharpe'][0]
            returns = i.loc[:,'returns'][0]
            turnover = i.loc[:,'turnover'][0]
            drawdown = i.loc[:,'drawdown'][0]
            alpha_id = i.loc[:,'alpha_id'][0]
            long_count = i.loc[:,'longCount'][0]
            short_count = i.loc[:,'shortCount'][0]

            if (long_count+short_count)<500 or short_count==0:
              pop_fitness.append(-1e9)
            elif turnover==0 or drawdown==0 or (long_count/short_count)<0.2 or (long_count/short_count)>5:
              pop_fitness.append(-1e9)
            elif sharpe>0 and returns>0:
              pop_fitness.append(((sharpe**2)*returns)/(((turnover+1.0))*drawdown))
            else:
              pop_fitness.append(-1 * (abs(sharpe**2)*abs(returns))/((turnover**(1/2))*drawdown))

            alpha = expressions[alpha_id]
            # print(alpha)
            new_alpha_pop.append(alpha)
            index+=1

          short_pop_size = len(short_alpha_pop)
          for i in range(short_pop_size):
            cur_ind = -1
            for j in range(pop_size):
              if compare(alpha_pop[j],new_alpha_pop[i])==True:
                cur_ind = j
                new_pop_fitness[cur_ind]=pop_fitness[i]
                fitness_cache[simplify(alpha_pop[j])]=pop_fitness[i]
            if cur_ind == -1:
              print("Fatal Error")


    old_index = 0
    for i in range(pop_size):
      if alpha_pop[i] in old_alpha_pop:
        new_pop_fitness[i] = old_pop_fitness[old_index]
        old_index+=1

    for i in range(pop_size):
      print(i,alpha_pop[i])
      print("Fitness: ", new_pop_fitness[i])

    return new_pop_fitness

def get_fittest_individual(alpha_pop, tree_pop, pop_fitness):
    best_fitness=-1e9
    best_tree = tree_pop[0]
    best_alpha = alpha_pop[0]

    index=-1
    for individual in tree_pop:
        index+=1
        f = pop_fitness[index]
        if f>best_fitness:
            best_fitness=f
            best_tree = individual
            best_alpha = alpha_pop[index]

    return best_fitness, best_tree, best_alpha

"""**Tournament Selection**


*   The below function selects the alphas based on their fitness value which are carried forward in the next generation and the ones used for crossover to generate the new population
*   Input: Tree population, Population size, Population fitness
*   Output: New tree population


"""

#@title Tournament Selection

tournament_size = 2

def comp_sort(e):
  return e[0]


def play_tournament(pop_fitness):
    winner_index = 0
    n = len(pop_fitness)
    temp = []
    for i in range(n):
      temp.append(i)
    players = random.sample(temp,tournament_size)
    winner = players[0]
    for i in range(tournament_size):
      if pop_fitness[players[i]] > pop_fitness[winner]:
        winner=players[i]
    return winner

def tournament_selection(tree_pop, pop_size, pop_fitness):
    new_tree_pop=[]
    zipped = list(zip(tree_pop, pop_fitness))
    random.shuffle(zipped)
    tree_pop, pop_fitness = zip(*zipped)

    zipped = list(zip(pop_fitness, tree_pop))
    zipped.sort(reverse=True,key=comp_sort)
    pop_fitness, tree_pop= zip(*zipped)

    refined_tree_pop=[]
    for i in range(len(tree_pop)):
      if len(refined_tree_pop)==(pop_size/2):
        break
      refined_tree_pop.append(tree_pop[i])

    for i in range(int((pop_size+0.1)/4)):
      parent1 = play_tournament(pop_fitness)
      parent2 = play_tournament(pop_fitness)
      child1,child2 = crossover(tree_pop[parent1],tree_pop[parent2])
      refined_tree_pop.append(child1)
      refined_tree_pop.append(child2)

    random.shuffle(refined_tree_pop)

    return refined_tree_pop

"""**Generating and Combining**


*   Below are defined two functions to randomly generate a tree and to finally generate tree population using the warm start method
*   generate_tree takes tree population and current depth as input, returns a tree of the given depth
*   generate_population_tree takes tree population, population size and current depth as input, returns the new tree population and new alpha population


"""

#@title Generating and Mixing
def generate_tree(tree_pop,cur_dep):
  all_ops = []
  all_ops = u_ops

  op_index = random.randint(0,len(all_ops)-1)
  selected_op = pick_op()

  outside_op_choice = random.randint(0,1)

  #outside operators preferred if depths are high
  if outside_op_choice == 1 and cur_dep>2:
    outside_op_index = random.randint(0,len(outside_ops)-1)
    selected_op = outside_ops[outside_op_index]

  individual = node(selected_op)
  i = random.randint(0,len(tree_pop)-1)
  selected_subtree = tree_pop[i]

  if selected_op in unary_ops:
    individual.left = selected_subtree
    return individual

  elif selected_op in ts_ops:
    individual.left = selected_subtree
    ts_flag_index = random.randint(0,len(ts_ops_flag)-1)
    individual.set_flag(ts_ops_flag[ts_flag_index])
    return individual

  elif selected_op in grp_ops:
    individual.left = selected_subtree
    grp_flag_index = random.randint(0,len(grp_ops_flag)-1)
    individual.set_flag(grp_ops_flag[grp_flag_index])
    return individual

  elif selected_op in trade_when_op:
    individual.left = selected_subtree
    trade_when_flag_index = random.randint(0,len(trade_when_op_flag)-1)
    individual.set_flag(trade_when_op_flag[trade_when_flag_index])
    return individual

  elif selected_op in hump_op:
    individual.left = selected_subtree
    hump_flag_index = random.randint(0,len(hump_op_flag)-1)
    individual.set_flag(hump_op_flag[hump_flag_index])
    return individual

  elif selected_op in truncate_op:
    individual.left = selected_subtree
    truncate_flag_index = random.randint(0,len(truncate_op_flag)-1)
    individual.set_flag(truncate_op_flag[truncate_flag_index])
    return individual

  else:
    i2 = random.randint(0,len(tree_pop)-1)
    selected_subtree2 = tree_pop[i2]
    individual.left = selected_subtree
    individual.right = selected_subtree2
    return individual

def generate_population_tree(tree_pop,pop_size,cur_dep):
  new_tree_pop = []
  new_alpha_pop = []
  k = 2
  for i in range(k*pop_size):
    individual = generate_tree(tree_pop,cur_dep)
    new_tree_pop.append(individual)
    new_alpha_pop.append(tree_to_alpha(individual))

  #warm start method
  new_pop_fitness = compute_fitness(new_alpha_pop, 0, 50, k*pop_size)

  zipped = list(zip(new_pop_fitness, new_tree_pop, new_alpha_pop))
  zipped.sort(reverse=True,key=comp_sort)
  new_pop_fitness, new_tree_pop, new_alpha_pop = zip(*zipped)

  refined_tree_pop=[]
  refined_alpha_pop=[]
  for i in range(len(new_tree_pop)):
    if len(refined_tree_pop)==pop_size:
      break
    if new_tree_pop[i] in refined_tree_pop:
      continue
    refined_alpha_pop.append(new_alpha_pop[i])
    refined_tree_pop.append(new_tree_pop[i])

  return refined_tree_pop, refined_alpha_pop

"""**Genetic Algorithm on Trees**



*   Below are defined two functions to implement the genetic algorithm and to run the algorithm for each depth
*   genetic_algorithm takes tree population, alpha population, total number of generations, population size and current depth as input, prints the best fit individual for each generation and returns the best fit tree population, alpha population and population fitness
*   genetic_algorithm_on_trees takes final depth, population size and total number of generations as input



"""

#@title Genetic Algorithm on Trees

data_backup={}

def genetic_algorithm(tree_pop, alpha_pop, gen_size, pop_size, cur_dep):
    gen = 0
    while gen!=gen_size:
        print(len(alpha_pop))
        pop_fitness = compute_fitness(alpha_pop, gen, gen_size, pop_size)
        best_fitness, best_tree, best_alpha = get_fittest_individual(alpha_pop, tree_pop, pop_fitness)
        print("")
        print("Current Depth: ", cur_dep)
        print("Generation: ",gen," Best Fitness: ",best_fitness, " Alpha: ", best_alpha, " Individual: ",best_tree)
        print("")
        gen+=1

        if(gen==gen_size):
            return tree_pop,alpha_pop,pop_fitness

        new_tree_pop =tournament_selection(tree_pop,pop_size, pop_fitness)
        for individual in new_tree_pop:
            if(cur_dep<2):
                individual=mutation(individual,0.45)
            else:
                individual=mutation(individual,0.25)

        tree_pop=new_tree_pop
        alpha_pop = [tree_to_alpha(x) for x in new_tree_pop]
        data_backup["Depth"]=cur_dep
        data_backup["Generation"]=gen
        data_backup["Tree_Population"]=tree_pop
        data_backup["Alpha_Population"]=alpha_pop

    return tree_pop,alpha_pop,pop_fitness


def genetic_algorithm_on_trees(depth_size,pop_size,no_of_generations):
  depth = 0
  tree_pop, alpha_pop = generate_pop(pop_size)
  while depth<depth_size:
    tree_pop, alpha_pop,pop_fitness = genetic_algorithm(tree_pop,alpha_pop,no_of_generations,pop_size, depth)
    depth+=1
    if depth==depth_size:
      break
    zipped = list(zip(pop_fitness, tree_pop))
    zipped.sort(reverse=True,key=comp_sort)
    new_pop_fitness, new_tree_pop = zip(*zipped)
    new_tree_pop = new_tree_pop[:pop_size]
    tree_pop, alpha_pop = generate_population_tree(new_tree_pop,pop_size,depth)

#Input: Total depth, population size, number of generations
"""
Example run: Before running this cell, change the variations in the variations cell above as done for the search algorithms
"""
genetic_algorithm_on_trees(3,20,2)

"""##**8. REFERENCES**

[Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment](https://www.researchgate.net/publication/372827826_Alpha-GPT_Human-AI_Interactive_Alpha_Mining_for_Quantitative_Investment#:~:text=Moreover%2C)

[AutoAlpha: an Efficient Hierarchical Evolutionary Algorithm for Mining Alpha Factors in Quantitative Investment](https://arxiv.org/pdf/2002.08245.pdf)
"""
